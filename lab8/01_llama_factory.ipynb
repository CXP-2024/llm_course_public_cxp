{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 8 Supervised Fine Tuning\n",
    "\n",
    "In this lab, we will perform parameter efficient finetuning (PEFT) to finetune a llama-2 model, using the HuggingFace SFTTrainer tool from its trl library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 15693148\n",
      "-rw-r--r-- 1 root root       7801 May 18  2024 LICENSE\n",
      "-rw-r--r-- 1 root root      37204 May 18  2024 README.md\n",
      "-rw-r--r-- 1 root root       4696 May 18  2024 USE_POLICY.md\n",
      "-rw-r--r-- 1 root root        654 May 18  2024 config.json\n",
      "-rw-r--r-- 1 root root         48 May 18  2024 configuration.json\n",
      "-rw-r--r-- 1 root root        187 May 18  2024 generation_config.json\n",
      "-rw-r--r-- 1 root root 4976698672 May 18  2024 model-00001-of-00004.safetensors\n",
      "-rw-r--r-- 1 root root 4999802720 May 18  2024 model-00002-of-00004.safetensors\n",
      "-rw-r--r-- 1 root root 4915916176 May 18  2024 model-00003-of-00004.safetensors\n",
      "-rw-r--r-- 1 root root 1168138808 May 18  2024 model-00004-of-00004.safetensors\n",
      "-rw-r--r-- 1 root root      23950 May 18  2024 model.safetensors.index.json\n",
      "-rw-r--r-- 1 root root         73 May 18  2024 special_tokens_map.json\n",
      "-rw-r--r-- 1 root root    9085698 May 18  2024 tokenizer.json\n",
      "-rw-r--r-- 1 root root      50982 May 18  2024 tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "# add proxy to access openai ...\n",
    "import os\n",
    "os.environ['HTTP_PROXY']=\"http://Clash:QOAF8Rmd@10.1.0.213:7890\"\n",
    "os.environ['HTTPS_PROXY']=\"http://Clash:QOAF8Rmd@10.1.0.213:7890\"\n",
    "os.environ['ALL_PROXY']=\"socks5://Clash:QOAF8Rmd@10.1.0.213:7893\"\n",
    "!ls -l /ssdshare/share/Meta-Llama-3-8B-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirrors.cernet.edu.cn/pypi/web/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: trl in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (0.9.6)\n",
      "Requirement already satisfied: fire in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (0.7.0)\n",
      "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (0.45.5)\n",
      "Requirement already satisfied: deepspeed in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (0.16.7)\n",
      "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from trl->-r requirements.txt (line 1)) (2.6.0)\n",
      "Requirement already satisfied: transformers>=4.31.0 in /usr/local/lib/python3.10/dist-packages (from trl->-r requirements.txt (line 1)) (4.51.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.18.2 in /usr/local/lib/python3.10/dist-packages (from trl->-r requirements.txt (line 1)) (1.26.4)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (from trl->-r requirements.txt (line 1)) (1.6.0)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from trl->-r requirements.txt (line 1)) (3.5.0)\n",
      "Requirement already satisfied: tyro>=0.5.11 in /usr/local/lib/python3.10/dist-packages (from trl->-r requirements.txt (line 1)) (0.8.14)\n",
      "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire->-r requirements.txt (line 2)) (3.1.0)\n",
      "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from deepspeed->-r requirements.txt (line 4)) (0.8.0)\n",
      "Requirement already satisfied: hjson in /usr/local/lib/python3.10/dist-packages (from deepspeed->-r requirements.txt (line 4)) (3.1.0)\n",
      "Requirement already satisfied: msgpack in /usr/local/lib/python3.10/dist-packages (from deepspeed->-r requirements.txt (line 4)) (1.0.8)\n",
      "Requirement already satisfied: ninja in /usr/local/lib/python3.10/dist-packages (from deepspeed->-r requirements.txt (line 4)) (1.11.1.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from deepspeed->-r requirements.txt (line 4)) (23.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from deepspeed->-r requirements.txt (line 4)) (6.0.0)\n",
      "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from deepspeed->-r requirements.txt (line 4)) (9.0.0)\n",
      "Requirement already satisfied: pydantic>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from deepspeed->-r requirements.txt (line 4)) (2.9.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from deepspeed->-r requirements.txt (line 4)) (4.66.5)\n",
      "Requirement already satisfied: nvidia-ml-py in /usr/local/lib/python3.10/dist-packages (from deepspeed->-r requirements.txt (line 4)) (12.575.51)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0.0->deepspeed->-r requirements.txt (line 4)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0.0->deepspeed->-r requirements.txt (line 4)) (2.23.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0.0->deepspeed->-r requirements.txt (line 4)) (4.12.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl->-r requirements.txt (line 1)) (3.16.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl->-r requirements.txt (line 1)) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl->-r requirements.txt (line 1)) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl->-r requirements.txt (line 1)) (2024.6.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl->-r requirements.txt (line 1)) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl->-r requirements.txt (line 1)) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl->-r requirements.txt (line 1)) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl->-r requirements.txt (line 1)) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl->-r requirements.txt (line 1)) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl->-r requirements.txt (line 1)) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl->-r requirements.txt (line 1)) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl->-r requirements.txt (line 1)) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl->-r requirements.txt (line 1)) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl->-r requirements.txt (line 1)) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl->-r requirements.txt (line 1)) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl->-r requirements.txt (line 1)) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl->-r requirements.txt (line 1)) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl->-r requirements.txt (line 1)) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl->-r requirements.txt (line 1)) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.4.0->trl->-r requirements.txt (line 1)) (1.3.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->trl->-r requirements.txt (line 1)) (0.30.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->trl->-r requirements.txt (line 1)) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->trl->-r requirements.txt (line 1)) (2024.9.11)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->trl->-r requirements.txt (line 1)) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->trl->-r requirements.txt (line 1)) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->trl->-r requirements.txt (line 1)) (0.4.5)\n",
      "Requirement already satisfied: docstring-parser>=0.16 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl->-r requirements.txt (line 1)) (0.16)\n",
      "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl->-r requirements.txt (line 1)) (13.7.1)\n",
      "Requirement already satisfied: shtab>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl->-r requirements.txt (line 1)) (1.7.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->trl->-r requirements.txt (line 1)) (16.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets->trl->-r requirements.txt (line 1)) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->trl->-r requirements.txt (line 1)) (2.2.2)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->trl->-r requirements.txt (line 1)) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets->trl->-r requirements.txt (line 1)) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->trl->-r requirements.txt (line 1)) (3.10.5)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl->-r requirements.txt (line 1)) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl->-r requirements.txt (line 1)) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl->-r requirements.txt (line 1)) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl->-r requirements.txt (line 1)) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl->-r requirements.txt (line 1)) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl->-r requirements.txt (line 1)) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl->-r requirements.txt (line 1)) (4.0.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.31.0->trl->-r requirements.txt (line 1)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.31.0->trl->-r requirements.txt (line 1)) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.31.0->trl->-r requirements.txt (line 1)) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.31.0->trl->-r requirements.txt (line 1)) (2024.8.30)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl->-r requirements.txt (line 1)) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl->-r requirements.txt (line 1)) (2.18.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.4.0->trl->-r requirements.txt (line 1)) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->trl->-r requirements.txt (line 1)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->trl->-r requirements.txt (line 1)) (2023.4)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->trl->-r requirements.txt (line 1)) (2024.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl->-r requirements.txt (line 1)) (0.1.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets->trl->-r requirements.txt (line 1)) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt\n",
    "\n",
    "#!mkdir -p /root/LLM-applications-course/lab8/LLaMA-Factory\n",
    "#!cd /root/LLM-applications-course/lab8/LLaMA-Factory/ && pip install -r /root/LLM-applications-course/lab8/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first change the working directory to /gfshome, to avoid writing too much data to the home directory. (Ignore the warnings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ln: failed to create symbolic link '/gfshome/Llama3-8B-Instruct-sft.yaml': File exists\n",
      "ln: failed to create symbolic link '/gfshome/Lora_Merge.yaml': File exists\n"
     ]
    }
   ],
   "source": [
    "# copy the config files to /gfshome, the working directory (will need later)\n",
    "!ln -s *.yaml /gfshome/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/gfshome\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd /gfshome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'LLaMA-Factory' already exists and is not an empty directory.\n",
      "Looking in indexes: https://mirrors.cernet.edu.cn/pypi/web/simple, https://pypi.ngc.nvidia.com\n",
      "Obtaining file:///gfshome/LLaMA-Factory\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing editable metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: transformers!=4.46.*,!=4.47.*,!=4.48.0,<=4.51.3,>=4.45.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (4.51.0)\n",
      "Requirement already satisfied: datasets<=3.5.0,>=2.16.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (3.5.0)\n",
      "Requirement already satisfied: accelerate<=1.6.0,>=0.34.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (1.6.0)\n",
      "Requirement already satisfied: peft<=0.15.1,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (0.15.1)\n",
      "Requirement already satisfied: trl<=0.9.6,>=0.8.6 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (0.9.6)\n",
      "Requirement already satisfied: tokenizers<=0.21.1,>=0.19.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (0.21.1)\n",
      "Requirement already satisfied: gradio<=5.25.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (5.23.3)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (1.14.0)\n",
      "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (0.8.0)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (0.2.0)\n",
      "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (0.9.0)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (5.29.4)\n",
      "Requirement already satisfied: uvicorn in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (0.34.0)\n",
      "Requirement already satisfied: fastapi in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (0.115.9)\n",
      "Requirement already satisfied: sse-starlette in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (2.3.5)\n",
      "Requirement already satisfied: matplotlib>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (3.9.2)\n",
      "Requirement already satisfied: fire in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (0.7.0)\n",
      "Requirement already satisfied: omegaconf in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (2.3.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (23.2)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (6.0.2)\n",
      "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (1.26.4)\n",
      "Requirement already satisfied: pydantic<=2.10.6 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (2.9.2)\n",
      "Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (2.2.2)\n",
      "Requirement already satisfied: av in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (14.3.0)\n",
      "Requirement already satisfied: librosa in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (0.10.1)\n",
      "Requirement already satisfied: tyro<0.9.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (0.8.14)\n",
      "Requirement already satisfied: torch>=1.13.1 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (2.6.0)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (3.9.1)\n",
      "Requirement already satisfied: jieba in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (0.42.1)\n",
      "Requirement already satisfied: rouge-chinese in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (1.0.3)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate<=1.6.0,>=0.34.0->llamafactory==0.9.3.dev0) (6.0.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate<=1.6.0,>=0.34.0->llamafactory==0.9.3.dev0) (0.30.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate<=1.6.0,>=0.34.0->llamafactory==0.9.3.dev0) (0.4.5)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (3.16.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (16.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (0.3.8)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (4.66.5)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (3.10.5)\n",
      "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (23.2.1)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (4.9.0)\n",
      "Requirement already satisfied: ffmpy in /usr/local/lib/python3.10/dist-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.5.0)\n",
      "Requirement already satisfied: gradio-client==1.8.0 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (1.8.0)\n",
      "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.1.2)\n",
      "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.27.2)\n",
      "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (3.1.6)\n",
      "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (2.1.5)\n",
      "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (3.10.16)\n",
      "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (10.4.0)\n",
      "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.25.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.0.20)\n",
      "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.11.4)\n",
      "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.1.6)\n",
      "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (2.10.0)\n",
      "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.45.3)\n",
      "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.13.2)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.12.5)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (4.12.2)\n",
      "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.8.0->gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (15.0.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (1.4.7)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->llamafactory==0.9.3.dev0) (2023.4)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->llamafactory==0.9.3.dev0) (2024.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<=2.10.6->llamafactory==0.9.3.dev0) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<=2.10.6->llamafactory==0.9.3.dev0) (2.23.4)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (3.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.13.1->llamafactory==0.9.3.dev0) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers!=4.46.*,!=4.47.*,!=4.48.0,<=4.51.3,>=4.45.0->llamafactory==0.9.3.dev0) (2024.9.11)\n",
      "Requirement already satisfied: docstring-parser>=0.16 in /usr/local/lib/python3.10/dist-packages (from tyro<0.9.0->llamafactory==0.9.3.dev0) (0.16)\n",
      "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.10/dist-packages (from tyro<0.9.0->llamafactory==0.9.3.dev0) (13.7.1)\n",
      "Requirement already satisfied: shtab>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from tyro<0.9.0->llamafactory==0.9.3.dev0) (1.7.2)\n",
      "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn->llamafactory==0.9.3.dev0) (8.1.7)\n",
      "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn->llamafactory==0.9.3.dev0) (0.14.0)\n",
      "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire->llamafactory==0.9.3.dev0) (3.1.0)\n",
      "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa->llamafactory==0.9.3.dev0) (3.0.1)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from librosa->llamafactory==0.9.3.dev0) (1.5.2)\n",
      "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa->llamafactory==0.9.3.dev0) (1.4.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa->llamafactory==0.9.3.dev0) (5.1.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa->llamafactory==0.9.3.dev0) (0.61.0)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from librosa->llamafactory==0.9.3.dev0) (0.12.1)\n",
      "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa->llamafactory==0.9.3.dev0) (1.8.2)\n",
      "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa->llamafactory==0.9.3.dev0) (0.5.0.post1)\n",
      "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa->llamafactory==0.9.3.dev0) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa->llamafactory==0.9.3.dev0) (1.0.8)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.10/dist-packages (from omegaconf->llamafactory==0.9.3.dev0) (4.9.3)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from rouge-chinese->llamafactory==0.9.3.dev0) (1.16.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (1.2.2)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (3.7)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (1.3.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (4.0.3)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (1.0.6)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa->llamafactory==0.9.3.dev0) (0.44.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.0->librosa->llamafactory==0.9.3.dev0) (4.3.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (2.0.7)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro<0.9.0->llamafactory==0.9.3.dev0) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro<0.9.0->llamafactory==0.9.3.dev0) (2.18.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->librosa->llamafactory==0.9.3.dev0) (3.5.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.12.1->librosa->llamafactory==0.9.3.dev0) (1.17.0)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (1.5.4)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa->llamafactory==0.9.3.dev0) (2.22)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro<0.9.0->llamafactory==0.9.3.dev0) (0.1.2)\n",
      "Building wheels for collected packages: llamafactory\n",
      "  Building editable for llamafactory (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for llamafactory: filename=llamafactory-0.9.3.dev0-0.editable-py3-none-any.whl size=27215 sha256=0dda102f60923b7bb23b996992fa621f13be160158dcf9f107dda7b1ddcff4e8\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-y56ye91w/wheels/87/26/82/8f4922c9e797dfc3e05b24c481d0e498ffae7c1e700eb2c667\n",
      "Successfully built llamafactory\n",
      "Installing collected packages: llamafactory\n",
      "  Attempting uninstall: llamafactory\n",
      "    Found existing installation: llamafactory 0.9.3.dev0\n",
      "    Uninstalling llamafactory-0.9.3.dev0:\n",
      "      Successfully uninstalled llamafactory-0.9.3.dev0\n",
      "Successfully installed llamafactory-0.9.3.dev0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#download llama factory\n",
    "!git clone https://github.com/hiyouga/LLaMA-Factory.git\n",
    "!cd LLaMA-Factory && pip install -e \".[torch,metrics]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Supervised Fine Tuning Example\n",
    "### 2.1 Motivation\n",
    "Llama3 is a versatile large language model available in various parameter sizes. Given its significant improvements in text generation tasks compared to its predecessor, Llama2, we aim to use Llama3-8B-Instruct to generate Chinese poetry based on specific themes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# Shared parameters between inference and SFT training\n",
    "################################################################################\n",
    "\n",
    "import transformers\n",
    "import torch\n",
    "# The base model\n",
    "model_name = \"/ssdshare/share/Meta-Llama-3-8B-Instruct\"\n",
    "# Use a single GPU\n",
    "# device_map = {'':0}\n",
    "# Use all GPUs\n",
    "device_map = \"auto\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# bitsandbytes parameters\n",
    "################################################################################\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit= True,    # use 4-bit precision for base model loading\n",
    "    bnb_4bit_quant_type= \"nf4\",  # Quantization type (fp4 or nf4)\n",
    "    bnb_4bit_compute_dtype= torch.bfloat16,   # Compute dtype for 4-bit base models  \"float16\" or torch.bfloat16\n",
    "    bnb_4bit_use_double_quant= False,  # Activate nested quantization for 4-bit base models (double quantization)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:50:55,513] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Warning: The cache directory for DeepSpeed Triton autotune, /root/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "WARNING: BNB_CUDA_VERSION=125 environment variable detected; loading libbitsandbytes_cuda125.so.\n",
      "This can be used to load a bitsandbytes version that is different from the PyTorch CUDA version.\n",
      "If this was unintended set the BNB_CUDA_VERSION variable to an empty string: export BNB_CUDA_VERSION=\n",
      "If you use the manual override make sure the right libcudart.so is in your LD_LIBRARY_PATH\n",
      "For example by adding the following to your .bashrc: export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:<path_to_cuda_dir/lib64\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44b3452ce60348179ac8c5abfbb31340",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    ")\n",
    "import os\n",
    "os.environ[\"BNB_CUDA_VERSION\"]=\"125\"\n",
    "# Load base model with bnb config\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "# Load LLaMA tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] Hi, you are a Chinese ancient poet, can you write a 2 sentence, 5-character poem about the theme of 风雨，旅人? [/INST]>\n",
      "\n",
      "</s>\n",
      "\n",
      "Here is a 2-sentence, 5-character poem about the theme of 风雨，旅人:\n",
      "\n",
      "风雨过客， \n",
      "旅人无家。\n",
      "\n",
      "\n",
      "Translation:\n",
      "\n",
      "The wind and rain pass by, \n",
      "The traveler has no home.\n",
      "\n",
      "\n",
      "In this poem, I used the theme of 风雨，旅人 to describe the struggles and hardships faced by travelers during a storm. The poem conveys a sense of loneliness and disconnection, as the traveler is forced to face the elements alone, with no place to call home. The imagery of wind and rain emphasizes the turmoil and uncertainty of the traveler's journey, while the phrase \"旅人无家\" (lǚ rén wú jiā) poignantly highlights the traveler's sense of displacement and isolation.\n"
     ]
    }
   ],
   "source": [
    "# Run text generation pipeline with our next model\n",
    "prompt = \"Hi, you are a Chinese ancient poet, can you write a 2 sentence, 5-character poem about the theme of 风雨，旅人?\" \n",
    "eos_ids = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\") \n",
    "]\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=200, eos_token_id=eos_ids, num_return_sequences=1)\n",
    "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output does not make any sense. Not only the number of characters in each line is not suffcient to our requirement, but also the tune and words used is not like ancient poet at all.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Preparing the training dataset\n",
    "\n",
    "Let's use sft to improve Llama3-8B-Instruct's ablity in this field now!\n",
    "\n",
    "You should prepare for the data we need to use for SFT in `02_poet data` .\n",
    "\n",
    "Please complete the procedures in that notebook.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 SFT with Llama-Factory\n",
    "\n",
    "For Processing SFT, we use llama factory, which is a highly modular, user-friendly platform with great ease of use, supporting distributed training and a variety of pre-trained models. Llama factory provide a WebUI to make it easy for using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:51:30,691] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Warning: The cache directory for DeepSpeed Triton autotune, /root/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.\n",
      "INFO 05-15 22:51:35 [__init__.py:239] Automatically detected platform cuda.\n",
      "Visit http://ip:port for Web UI, e.g., http://127.0.0.1:7860\n",
      "* Running on local URL:  http://0.0.0.0:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n",
      "[2025-05-15 22:56:21,773] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Warning: The cache directory for DeepSpeed Triton autotune, /root/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.\n",
      "INFO 05-15 22:56:26 [__init__.py:239] Automatically detected platform cuda.\n",
      "[INFO|2025-05-15 22:56:31] llamafactory.cli:143 >> Initializing 2 distributed tasks at: 127.0.0.1:50517\n",
      "W0515 22:56:32.906000 71085 torch/distributed/run.py:792] \n",
      "W0515 22:56:32.906000 71085 torch/distributed/run.py:792] *****************************************\n",
      "W0515 22:56:32.906000 71085 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W0515 22:56:32.906000 71085 torch/distributed/run.py:792] *****************************************\n",
      "[2025-05-15 22:56:38,358] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-05-15 22:56:38,428] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Warning: The cache directory for DeepSpeed Triton autotune, /root/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.\n",
      "Warning: The cache directory for DeepSpeed Triton autotune, /root/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.\n",
      "[2025-05-15 22:56:43,511] [INFO] [comm.py:669:init_distributed] cdb=None\n",
      "[2025-05-15 22:56:43,512] [INFO] [comm.py:669:init_distributed] cdb=None\n",
      "[2025-05-15 22:56:43,512] [INFO] [comm.py:700:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[WARNING|2025-05-15 22:56:43] llamafactory.hparams.parser:148 >> We recommend enable `upcast_layernorm` in quantized training.\n",
      "[INFO|2025-05-15 22:56:43] llamafactory.hparams.parser:143 >> Set `ddp_find_unused_parameters` to False in DDP training since LoRA is enabled.\n",
      "[INFO|2025-05-15 22:56:43] llamafactory.hparams.parser:401 >> Process rank: 0, world size: 2, device: cuda:0, distributed training: True, compute dtype: torch.bfloat16\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-05-15 22:56:43,829 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-05-15 22:56:43,830 >> loading file tokenizer.model\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-05-15 22:56:43,830 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-05-15 22:56:43,830 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-05-15 22:56:43,830 >> loading file tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-05-15 22:56:43,830 >> loading file chat_template.jinja\n",
      "[INFO|2025-05-15 22:56:43] llamafactory.hparams.parser:401 >> Process rank: 1, world size: 2, device: cuda:1, distributed training: True, compute dtype: torch.bfloat16\n",
      "[INFO|tokenization_utils_base.py:2323] 2025-05-15 22:56:44,329 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|configuration_utils.py:691] 2025-05-15 22:56:44,335 >> loading configuration file /ssdshare/share/Meta-Llama-3-8B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:765] 2025-05-15 22:56:44,337 >> Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-05-15 22:56:44,341 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-05-15 22:56:44,341 >> loading file tokenizer.model\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-05-15 22:56:44,341 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-05-15 22:56:44,341 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-05-15 22:56:44,341 >> loading file tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-05-15 22:56:44,341 >> loading file chat_template.jinja\n",
      "[INFO|tokenization_utils_base.py:2323] 2025-05-15 22:56:44,771 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|2025-05-15 22:56:44] llamafactory.data.template:143 >> Replace eos token: <|eot_id|>.\n",
      "[INFO|2025-05-15 22:56:44] llamafactory.data.template:143 >> Add pad token: <|eot_id|>\n",
      "[INFO|2025-05-15 22:56:44] llamafactory.data.template:143 >> Add <|eom_id|> to stop words.\n",
      "[WARNING|2025-05-15 22:56:44] llamafactory.data.template:148 >> New tokens have been added, make sure `resize_vocab` is True.\n",
      "[INFO|2025-05-15 22:56:44] llamafactory.data.loader:143 >> Loading dataset alpaca_sft_dataset_with_varied_instructions.json...\n",
      "[rank1]:[W515 22:56:44.435070227 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\n",
      "Setting num_proc from 16 back to 1 for the train split to disable multiprocessing as it only contains one shard.\n",
      "Generating train split: 109727 examples [00:00, 112643.32 examples/s]\n",
      "Converting format of dataset (num_proc=16): 100%|█| 100000/100000 [00:00<00:00, \n",
      "[rank0]:[W515 22:56:48.678456456 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\n",
      "Running tokenizer on dataset (num_proc=16): 100%|█| 100000/100000 [00:05<00:00, \n",
      "training example:\n",
      "input_ids:\n",
      "[128000, 128006, 882, 128007, 271, 13347, 11, 439, 264, 8620, 40360, 11, 649, 499, 1520, 757, 311, 1893, 264, 220, 20, 80325, 220, 19, 8614, 33894, 430, 52924, 279, 22100, 315, 80721, 237, 30867, 11, 122438, 9080, 11, 123335, 103125, 11, 10447, 3299, 51043, 242, 30, 128009, 128006, 78191, 128007, 271, 103553, 244, 162, 7644, 102718, 110261, 102491, 42553, 28190, 21403, 232, 15120, 114897, 30867, 9174, 116366, 9080, 89753, 110588, 105301, 42553, 104149, 103125, 46961, 114735, 51043, 242, 1811, 128009]\n",
      "inputs:\n",
      "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Hi, as a Chinese poet, can you help me to create a 5-character 4-line poem that incorporates the themes of 屏开, 晴日, 春风, 绿苔?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "崖悬百尺古，\n",
      "面削一屏开。\n",
      "晴日流丹草，\n",
      "春风长绿苔。<|eot_id|>\n",
      "label_ids:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 103553, 244, 162, 7644, 102718, 110261, 102491, 42553, 28190, 21403, 232, 15120, 114897, 30867, 9174, 116366, 9080, 89753, 110588, 105301, 42553, 104149, 103125, 46961, 114735, 51043, 242, 1811, 128009]\n",
      "labels:\n",
      "崖悬百尺古，\n",
      "面削一屏开。\n",
      "晴日流丹草，\n",
      "春风长绿苔。<|eot_id|>\n",
      "[INFO|configuration_utils.py:691] 2025-05-15 22:56:54,607 >> loading configuration file /ssdshare/share/Meta-Llama-3-8B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:765] 2025-05-15 22:56:54,609 >> Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|2025-05-15 22:56:54] llamafactory.model.model_utils.quantization:143 >> Quantizing model to 4 bit with bitsandbytes.\n",
      "[INFO|2025-05-15 22:56:54] llamafactory.model.model_utils.kv_cache:143 >> KV cache is disabled during training.\n",
      "WARNING: BNB_CUDA_VERSION=125 environment variable detected; loading libbitsandbytes_cuda125.so.\n",
      "This can be used to load a bitsandbytes version that is different from the PyTorch CUDA version.\n",
      "If this was unintended set the BNB_CUDA_VERSION variable to an empty string: export BNB_CUDA_VERSION=\n",
      "If you use the manual override make sure the right libcudart.so is in your LD_LIBRARY_PATH\n",
      "For example by adding the following to your .bashrc: export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:<path_to_cuda_dir/lib64\n",
      "\n",
      "[INFO|modeling_utils.py:1121] 2025-05-15 22:56:54,785 >> loading weights file /ssdshare/share/Meta-Llama-3-8B-Instruct/model.safetensors.index.json\n",
      "[INFO|modeling_utils.py:2167] 2025-05-15 22:56:54,786 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
      "[INFO|configuration_utils.py:1142] 2025-05-15 22:56:54,789 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"use_cache\": false\n",
      "}\n",
      "\n",
      "WARNING: BNB_CUDA_VERSION=125 environment variable detected; loading libbitsandbytes_cuda125.so.\n",
      "This can be used to load a bitsandbytes version that is different from the PyTorch CUDA version.\n",
      "If this was unintended set the BNB_CUDA_VERSION variable to an empty string: export BNB_CUDA_VERSION=\n",
      "If you use the manual override make sure the right libcudart.so is in your LD_LIBRARY_PATH\n",
      "For example by adding the following to your .bashrc: export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:<path_to_cuda_dir/lib64\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [00:13<00:00,  3.43s/it]\n",
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [00:14<00:00,  3.57s/it]\n",
      "[INFO|modeling_utils.py:4926] 2025-05-15 22:57:09,227 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4934] 2025-05-15 22:57:09,228 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /ssdshare/share/Meta-Llama-3-8B-Instruct.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:1095] 2025-05-15 22:57:09,232 >> loading configuration file /ssdshare/share/Meta-Llama-3-8B-Instruct/generation_config.json\n",
      "[INFO|configuration_utils.py:1142] 2025-05-15 22:57:09,232 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128009\n",
      "  ],\n",
      "  \"max_length\": 4096,\n",
      "  \"temperature\": 0.6,\n",
      "  \"top_p\": 0.9\n",
      "}\n",
      "\n",
      "[INFO|2025-05-15 22:57:09] llamafactory.model.model_utils.checkpointing:143 >> Gradient checkpointing enabled.\n",
      "[INFO|2025-05-15 22:57:09] llamafactory.model.model_utils.attention:143 >> Using torch SDPA for faster training and inference.\n",
      "[INFO|2025-05-15 22:57:09] llamafactory.model.adapter:143 >> Upcasting trainable params to float32.\n",
      "[INFO|2025-05-15 22:57:09] llamafactory.model.adapter:143 >> Fine-tuning method: LoRA\n",
      "[INFO|2025-05-15 22:57:09] llamafactory.model.model_utils.misc:143 >> Found linear modules: k_proj,gate_proj,v_proj,up_proj,o_proj,q_proj,down_proj\n",
      "[INFO|2025-05-15 22:57:11] llamafactory.model.loader:143 >> trainable params: 167,772,160 || all params: 8,198,033,408 || trainable%: 2.0465\n",
      "[INFO|trainer.py:748] 2025-05-15 22:57:11,202 >> Using auto half precision backend\n",
      "[2025-05-15 22:57:11,598] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed info: version=0.16.7, git-hash=unknown, git-branch=unknown\n",
      "[2025-05-15 22:57:11,598] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 2\n",
      "[2025-05-15 22:57:11,748] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2025-05-15 22:57:11,753] [INFO] [logging.py:107:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2025-05-15 22:57:11,753] [INFO] [logging.py:107:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2025-05-15 22:57:11,795] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW\n",
      "[2025-05-15 22:57:11,795] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'bitsandbytes.optim.adamw.AdamW'>\n",
      "[2025-05-15 22:57:11,795] [WARNING] [engine.py:1338:_do_optimizer_sanity_check] **** You are using ZeRO with an untested optimizer, proceed with caution *****\n",
      "[2025-05-15 22:57:11,795] [INFO] [logging.py:107:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2025-05-15 22:57:11,795] [INFO] [stage_1_and_2.py:150:__init__] Reduce bucket size 500000000\n",
      "[2025-05-15 22:57:11,795] [INFO] [stage_1_and_2.py:151:__init__] Allgather bucket size 500000000\n",
      "[2025-05-15 22:57:11,795] [INFO] [stage_1_and_2.py:152:__init__] CPU Offload: False\n",
      "[2025-05-15 22:57:11,795] [INFO] [stage_1_and_2.py:153:__init__] Round robin gradient partitioning: True\n",
      "[2025-05-15 22:57:13,685] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states\n",
      "[2025-05-15 22:57:13,686] [INFO] [utils.py:782:see_memory_usage] MA 5.94 GB         Max_MA 6.09 GB         CA 6.16 GB         Max_CA 6 GB \n",
      "[2025-05-15 22:57:13,686] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 55.04 GB, percent = 5.5%\n",
      "[2025-05-15 22:57:13,919] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states\n",
      "[2025-05-15 22:57:13,920] [INFO] [utils.py:782:see_memory_usage] MA 5.94 GB         Max_MA 6.25 GB         CA 6.47 GB         Max_CA 6 GB \n",
      "[2025-05-15 22:57:13,920] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 54.93 GB, percent = 5.5%\n",
      "[2025-05-15 22:57:13,920] [INFO] [stage_1_and_2.py:557:__init__] optimizer state initialized\n",
      "[2025-05-15 22:57:14,155] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2025-05-15 22:57:14,155] [INFO] [utils.py:782:see_memory_usage] MA 5.94 GB         Max_MA 5.94 GB         CA 6.47 GB         Max_CA 6 GB \n",
      "[2025-05-15 22:57:14,156] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 54.94 GB, percent = 5.5%\n",
      "[2025-05-15 22:57:14,161] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer\n",
      "[2025-05-15 22:57:14,161] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = None\n",
      "[2025-05-15 22:57:14,161] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2025-05-15 22:57:14,162] [INFO] [logging.py:107:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0], mom=[(0.9, 0.999)]\n",
      "[2025-05-15 22:57:14,169] [INFO] [config.py:1003:print] DeepSpeedEngine configuration:\n",
      "[2025-05-15 22:57:14,170] [INFO] [config.py:1007:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2025-05-15 22:57:14,170] [INFO] [config.py:1007:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'intra_op_parallelism': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}\n",
      "[2025-05-15 22:57:14,170] [INFO] [config.py:1007:print]   amp_enabled .................. False\n",
      "[2025-05-15 22:57:14,170] [INFO] [config.py:1007:print]   amp_params ................... False\n",
      "[2025-05-15 22:57:14,170] [INFO] [config.py:1007:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2025-05-15 22:57:14,170] [INFO] [config.py:1007:print]   bfloat16_enabled ............. True\n",
      "[2025-05-15 22:57:14,170] [INFO] [config.py:1007:print]   bfloat16_immediate_grad_update  True\n",
      "[2025-05-15 22:57:14,170] [INFO] [config.py:1007:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2025-05-15 22:57:14,170] [INFO] [config.py:1007:print]   checkpoint_tag_validation_enabled  True\n",
      "[2025-05-15 22:57:14,170] [INFO] [config.py:1007:print]   checkpoint_tag_validation_fail  False\n",
      "[2025-05-15 22:57:14,170] [INFO] [config.py:1007:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f242cef41c0>\n",
      "[2025-05-15 22:57:14,170] [INFO] [config.py:1007:print]   communication_data_type ...... None\n",
      "[2025-05-15 22:57:14,170] [INFO] [config.py:1007:print]   compile_config ............... deepcompile=False free_activation=False offload_activation=False offload_opt_states=False double_buffer=True symmetric_memory=False debug_log=False offload_parameters=False sync_before_reduce=False sync_after_reduce=False sync_before_allgather=False sync_after_allgather=False\n",
      "[2025-05-15 22:57:14,170] [INFO] [config.py:1007:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2025-05-15 22:57:14,170] [INFO] [config.py:1007:print]   curriculum_enabled_legacy .... False\n",
      "[2025-05-15 22:57:14,170] [INFO] [config.py:1007:print]   curriculum_params_legacy ..... False\n",
      "[2025-05-15 22:57:14,170] [INFO] [config.py:1007:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'pin_memory': False, 'curriculum_learning': {'enabled': False}, 'dynamic_batching': {'enabled': False, 'lr_scaling_method': 'linear', 'min_batch_size': 1, 'max_batch_size': None, 'sequence_picking_order': 'dataloader', 'verbose': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2025-05-15 22:57:14,170] [INFO] [config.py:1007:print]   data_efficiency_enabled ...... False\n",
      "[2025-05-15 22:57:14,170] [INFO] [config.py:1007:print]   dataloader_drop_last ......... False\n",
      "[2025-05-15 22:57:14,170] [INFO] [config.py:1007:print]   disable_allgather ............ False\n",
      "[2025-05-15 22:57:14,171] [INFO] [config.py:1007:print]   dump_state ................... False\n",
      "[2025-05-15 22:57:14,171] [INFO] [config.py:1007:print]   dynamic_loss_scale_args ...... None\n",
      "[2025-05-15 22:57:14,171] [INFO] [config.py:1007:print]   eigenvalue_enabled ........... False\n",
      "[2025-05-15 22:57:14,171] [INFO] [config.py:1007:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2025-05-15 22:57:14,171] [INFO] [config.py:1007:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2025-05-15 22:57:14,171] [INFO] [config.py:1007:print]   eigenvalue_layer_num ......... 0\n",
      "[2025-05-15 22:57:14,171] [INFO] [config.py:1007:print]   eigenvalue_max_iter .......... 100\n",
      "[2025-05-15 22:57:14,171] [INFO] [config.py:1007:print]   eigenvalue_stability ......... 1e-06\n",
      "[2025-05-15 22:57:14,171] [INFO] [config.py:1007:print]   eigenvalue_tol ............... 0.01\n",
      "[2025-05-15 22:57:14,171] [INFO] [config.py:1007:print]   eigenvalue_verbose ........... False\n",
      "[2025-05-15 22:57:14,171] [INFO] [config.py:1007:print]   elasticity_enabled ........... False\n",
      "[2025-05-15 22:57:14,171] [INFO] [config.py:1007:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2025-05-15 22:57:14,171] [INFO] [config.py:1007:print]   fp16_auto_cast ............... None\n",
      "[2025-05-15 22:57:14,171] [INFO] [config.py:1007:print]   fp16_enabled ................. False\n",
      "[2025-05-15 22:57:14,171] [INFO] [config.py:1007:print]   fp16_master_weights_and_gradients  False\n",
      "[2025-05-15 22:57:14,171] [INFO] [config.py:1007:print]   global_rank .................. 0\n",
      "[2025-05-15 22:57:14,171] [INFO] [config.py:1007:print]   grad_accum_dtype ............. None\n",
      "[2025-05-15 22:57:14,171] [INFO] [config.py:1007:print]   gradient_accumulation_steps .. 1\n",
      "[2025-05-15 22:57:14,171] [INFO] [config.py:1007:print]   gradient_clipping ............ 0.3\n",
      "[2025-05-15 22:57:14,171] [INFO] [config.py:1007:print]   gradient_predivide_factor .... 1.0\n",
      "[2025-05-15 22:57:14,171] [INFO] [config.py:1007:print]   graph_harvesting ............. False\n",
      "[2025-05-15 22:57:14,171] [INFO] [config.py:1007:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2025-05-15 22:57:14,171] [INFO] [config.py:1007:print]   initial_dynamic_scale ........ 1\n",
      "[2025-05-15 22:57:14,171] [INFO] [config.py:1007:print]   load_universal_checkpoint .... False\n",
      "[2025-05-15 22:57:14,171] [INFO] [config.py:1007:print]   loss_scale ................... 1.0\n",
      "[2025-05-15 22:57:14,171] [INFO] [config.py:1007:print]   memory_breakdown ............. False\n",
      "[2025-05-15 22:57:14,171] [INFO] [config.py:1007:print]   mics_hierarchial_params_gather  False\n",
      "[2025-05-15 22:57:14,171] [INFO] [config.py:1007:print]   mics_shard_size .............. -1\n",
      "[2025-05-15 22:57:14,171] [INFO] [config.py:1007:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')\n",
      "[2025-05-15 22:57:14,171] [INFO] [config.py:1007:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2025-05-15 22:57:14,171] [INFO] [config.py:1007:print]   optimizer_legacy_fusion ...... False\n",
      "[2025-05-15 22:57:14,171] [INFO] [config.py:1007:print]   optimizer_name ............... None\n",
      "[2025-05-15 22:57:14,171] [INFO] [config.py:1007:print]   optimizer_params ............. None\n",
      "[2025-05-15 22:57:14,171] [INFO] [config.py:1007:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2025-05-15 22:57:14,171] [INFO] [config.py:1007:print]   pld_enabled .................. False\n",
      "[2025-05-15 22:57:14,171] [INFO] [config.py:1007:print]   pld_params ................... False\n",
      "[2025-05-15 22:57:14,171] [INFO] [config.py:1007:print]   prescale_gradients ........... False\n",
      "[2025-05-15 22:57:14,172] [INFO] [config.py:1007:print]   scheduler_name ............... None\n",
      "[2025-05-15 22:57:14,172] [INFO] [config.py:1007:print]   scheduler_params ............. None\n",
      "[2025-05-15 22:57:14,172] [INFO] [config.py:1007:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2025-05-15 22:57:14,172] [INFO] [config.py:1007:print]   sparse_attention ............. None\n",
      "[2025-05-15 22:57:14,172] [INFO] [config.py:1007:print]   sparse_gradients_enabled ..... False\n",
      "[2025-05-15 22:57:14,172] [INFO] [config.py:1007:print]   steps_per_print .............. inf\n",
      "[2025-05-15 22:57:14,172] [INFO] [config.py:1007:print]   tensor_parallel_config ....... dtype=torch.float16 autotp_size=0 tp_overlap_comm=False tensor_parallel=TPConfig(tp_size=1, tp_grain_size=1, mpu=None, tp_group=None) injection_policy_tuple=None keep_module_on_host=False replace_with_kernel_inject=False\n",
      "[2025-05-15 22:57:14,172] [INFO] [config.py:1007:print]   timers_config ................ enabled=True synchronized=True\n",
      "[2025-05-15 22:57:14,172] [INFO] [config.py:1007:print]   train_batch_size ............. 16\n",
      "[2025-05-15 22:57:14,172] [INFO] [config.py:1007:print]   train_micro_batch_size_per_gpu  8\n",
      "[2025-05-15 22:57:14,172] [INFO] [config.py:1007:print]   use_data_before_expert_parallel_  False\n",
      "[2025-05-15 22:57:14,172] [INFO] [config.py:1007:print]   use_node_local_storage ....... False\n",
      "[2025-05-15 22:57:14,172] [INFO] [config.py:1007:print]   wall_clock_breakdown ......... False\n",
      "[2025-05-15 22:57:14,172] [INFO] [config.py:1007:print]   weight_quantization_config ... None\n",
      "[2025-05-15 22:57:14,172] [INFO] [config.py:1007:print]   world_size ................... 2\n",
      "[2025-05-15 22:57:14,172] [INFO] [config.py:1007:print]   zero_allow_untested_optimizer  True\n",
      "[2025-05-15 22:57:14,172] [INFO] [config.py:1007:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=True zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True log_trace_cache_warnings=False\n",
      "[2025-05-15 22:57:14,172] [INFO] [config.py:1007:print]   zero_enabled ................. True\n",
      "[2025-05-15 22:57:14,172] [INFO] [config.py:1007:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2025-05-15 22:57:14,172] [INFO] [config.py:1007:print]   zero_optimization_stage ...... 2\n",
      "[2025-05-15 22:57:14,172] [INFO] [config.py:993:print_user_config]   json = {\n",
      "    \"train_batch_size\": 16, \n",
      "    \"train_micro_batch_size_per_gpu\": 8, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"gradient_clipping\": 0.3, \n",
      "    \"zero_allow_untested_optimizer\": true, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false, \n",
      "        \"loss_scale\": 0, \n",
      "        \"loss_scale_window\": 1000, \n",
      "        \"initial_scale_power\": 16, \n",
      "        \"hysteresis\": 2, \n",
      "        \"min_loss_scale\": 1\n",
      "    }, \n",
      "    \"bf16\": {\n",
      "        \"enabled\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 5.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 5.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"round_robin_gradients\": true\n",
      "    }, \n",
      "    \"steps_per_print\": inf\n",
      "}\n",
      "[INFO|trainer.py:2414] 2025-05-15 22:57:14,174 >> ***** Running training *****\n",
      "[INFO|trainer.py:2415] 2025-05-15 22:57:14,174 >>   Num examples = 100,000\n",
      "[INFO|trainer.py:2416] 2025-05-15 22:57:14,174 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:2417] 2025-05-15 22:57:14,174 >>   Instantaneous batch size per device = 8\n",
      "[INFO|trainer.py:2420] 2025-05-15 22:57:14,174 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "[INFO|trainer.py:2421] 2025-05-15 22:57:14,174 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:2422] 2025-05-15 22:57:14,174 >>   Total optimization steps = 2,063\n",
      "[INFO|trainer.py:2423] 2025-05-15 22:57:14,179 >>   Number of trainable parameters = 167,772,160\n",
      "  5%|█▉                                      | 100/2063 [01:45<33:10,  1.01s/it][INFO|2025-05-15 22:58:59] llamafactory.train.callbacks:143 >> {'loss': 3.8397, 'learning_rate': 9.9000e-05, 'epoch': 0.02, 'throughput': 1433.17}\n",
      "{'loss': 3.8397, 'grad_norm': 0.8474973440170288, 'learning_rate': 9.900000000000001e-05, 'epoch': 0.02, 'num_input_tokens_seen': 150784}\n",
      " 10%|███▉                                    | 200/2063 [03:30<32:57,  1.06s/it][INFO|2025-05-15 23:00:44] llamafactory.train.callbacks:143 >> {'loss': 3.3476, 'learning_rate': 1.9900e-04, 'epoch': 0.03, 'throughput': 1435.14}\n",
      "{'loss': 3.3476, 'grad_norm': 0.7469322085380554, 'learning_rate': 0.000199, 'epoch': 0.03, 'num_input_tokens_seen': 301888}\n",
      " 10%|███▉                                    | 200/2063 [03:30<32:57,  1.06s/it][INFO|trainer.py:3984] 2025-05-15 23:00:48,231 >> Saving model checkpoint to saves/Llama-3-8B-Instruct/lora/train_2025-05-15-22-51-42/checkpoint-200\n",
      "[INFO|configuration_utils.py:691] 2025-05-15 23:00:48,303 >> loading configuration file /ssdshare/share/Meta-Llama-3-8B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:765] 2025-05-15 23:00:48,304 >> Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2510] 2025-05-15 23:00:51,484 >> tokenizer config file saved in saves/Llama-3-8B-Instruct/lora/train_2025-05-15-22-51-42/checkpoint-200/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2519] 2025-05-15 23:00:51,495 >> Special tokens file saved in saves/Llama-3-8B-Instruct/lora/train_2025-05-15-22-51-42/checkpoint-200/special_tokens_map.json\n",
      "[2025-05-15 23:00:52,019] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step200 is about to be saved!\n",
      "[2025-05-15 23:00:52,560] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: saves/Llama-3-8B-Instruct/lora/train_2025-05-15-22-51-42/checkpoint-200/global_step200/mp_rank_00_model_states.pt\n",
      "[2025-05-15 23:00:52,560] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Llama-3-8B-Instruct/lora/train_2025-05-15-22-51-42/checkpoint-200/global_step200/mp_rank_00_model_states.pt...\n",
      "[2025-05-15 23:00:56,979] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Llama-3-8B-Instruct/lora/train_2025-05-15-22-51-42/checkpoint-200/global_step200/mp_rank_00_model_states.pt.\n",
      "[2025-05-15 23:00:57,000] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Llama-3-8B-Instruct/lora/train_2025-05-15-22-51-42/checkpoint-200/global_step200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...\n",
      "[2025-05-15 23:01:06,595] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Llama-3-8B-Instruct/lora/train_2025-05-15-22-51-42/checkpoint-200/global_step200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.\n",
      "[2025-05-15 23:01:06,653] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/Llama-3-8B-Instruct/lora/train_2025-05-15-22-51-42/checkpoint-200/global_step200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt\n",
      "[2025-05-15 23:01:06,653] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step200 is ready now!\n",
      " 15%|█████▊                                  | 300/2063 [05:36<30:25,  1.04s/it][INFO|2025-05-15 23:02:50] llamafactory.train.callbacks:143 >> {'loss': 3.2794, 'learning_rate': 1.9861e-04, 'epoch': 0.05, 'throughput': 1347.39}\n",
      "{'loss': 3.2794, 'grad_norm': 0.6885935664176941, 'learning_rate': 0.0001986097095854347, 'epoch': 0.05, 'num_input_tokens_seen': 453120}\n",
      " 19%|███████▊                                | 400/2063 [07:17<28:05,  1.01s/it][INFO|2025-05-15 23:04:31] llamafactory.train.callbacks:143 >> {'loss': 3.2015, 'learning_rate': 1.9442e-04, 'epoch': 0.06, 'throughput': 1381.78}\n",
      "{'loss': 3.2015, 'grad_norm': 0.7184101343154907, 'learning_rate': 0.00019442209852030647, 'epoch': 0.06, 'num_input_tokens_seen': 604160}\n",
      " 19%|███████▊                                | 400/2063 [07:17<28:05,  1.01s/it][INFO|trainer.py:3984] 2025-05-15 23:04:35,190 >> Saving model checkpoint to saves/Llama-3-8B-Instruct/lora/train_2025-05-15-22-51-42/checkpoint-400\n",
      "[INFO|configuration_utils.py:691] 2025-05-15 23:04:35,256 >> loading configuration file /ssdshare/share/Meta-Llama-3-8B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:765] 2025-05-15 23:04:35,257 >> Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2510] 2025-05-15 23:04:38,105 >> tokenizer config file saved in saves/Llama-3-8B-Instruct/lora/train_2025-05-15-22-51-42/checkpoint-400/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2519] 2025-05-15 23:04:38,114 >> Special tokens file saved in saves/Llama-3-8B-Instruct/lora/train_2025-05-15-22-51-42/checkpoint-400/special_tokens_map.json\n",
      "[2025-05-15 23:04:38,628] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step400 is about to be saved!\n",
      "[2025-05-15 23:04:39,184] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: saves/Llama-3-8B-Instruct/lora/train_2025-05-15-22-51-42/checkpoint-400/global_step400/mp_rank_00_model_states.pt\n",
      "[2025-05-15 23:04:39,184] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Llama-3-8B-Instruct/lora/train_2025-05-15-22-51-42/checkpoint-400/global_step400/mp_rank_00_model_states.pt...\n",
      "[2025-05-15 23:04:43,445] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Llama-3-8B-Instruct/lora/train_2025-05-15-22-51-42/checkpoint-400/global_step400/mp_rank_00_model_states.pt.\n",
      "[2025-05-15 23:04:43,491] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Llama-3-8B-Instruct/lora/train_2025-05-15-22-51-42/checkpoint-400/global_step400/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...\n",
      "[2025-05-15 23:04:52,582] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Llama-3-8B-Instruct/lora/train_2025-05-15-22-51-42/checkpoint-400/global_step400/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.\n",
      "[2025-05-15 23:04:52,605] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/Llama-3-8B-Instruct/lora/train_2025-05-15-22-51-42/checkpoint-400/global_step400/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt\n",
      "[2025-05-15 23:04:52,605] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step400 is ready now!\n",
      " 24%|█████████▋                              | 500/2063 [09:23<27:13,  1.05s/it][INFO|2025-05-15 23:06:38] llamafactory.train.callbacks:143 >> {'loss': 3.1494, 'learning_rate': 1.8756e-04, 'epoch': 0.08, 'throughput': 1338.13}\n",
      "{'loss': 3.1494, 'grad_norm': 0.713478147983551, 'learning_rate': 0.0001875558231302091, 'epoch': 0.08, 'num_input_tokens_seen': 754624}\n",
      " 29%|███████████▋                            | 600/2063 [11:08<25:03,  1.03s/it][INFO|2025-05-15 23:08:23] llamafactory.train.callbacks:143 >> {'loss': 3.1139, 'learning_rate': 1.7821e-04, 'epoch': 0.10, 'throughput': 1353.86}\n",
      "{'loss': 3.1139, 'grad_norm': 0.6968201994895935, 'learning_rate': 0.00017820567305889228, 'epoch': 0.1, 'num_input_tokens_seen': 905728}\n",
      " 29%|███████████▋                            | 600/2063 [11:08<25:03,  1.03s/it][INFO|trainer.py:3984] 2025-05-15 23:08:26,765 >> Saving model checkpoint to saves/Llama-3-8B-Instruct/lora/train_2025-05-15-22-51-42/checkpoint-600\n",
      "[INFO|configuration_utils.py:691] 2025-05-15 23:08:26,830 >> loading configuration file /ssdshare/share/Meta-Llama-3-8B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:765] 2025-05-15 23:08:26,831 >> Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2510] 2025-05-15 23:08:30,037 >> tokenizer config file saved in saves/Llama-3-8B-Instruct/lora/train_2025-05-15-22-51-42/checkpoint-600/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2519] 2025-05-15 23:08:30,047 >> Special tokens file saved in saves/Llama-3-8B-Instruct/lora/train_2025-05-15-22-51-42/checkpoint-600/special_tokens_map.json\n",
      "[2025-05-15 23:08:30,607] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step600 is about to be saved!\n",
      "[2025-05-15 23:08:31,120] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: saves/Llama-3-8B-Instruct/lora/train_2025-05-15-22-51-42/checkpoint-600/global_step600/mp_rank_00_model_states.pt\n",
      "[2025-05-15 23:08:31,120] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Llama-3-8B-Instruct/lora/train_2025-05-15-22-51-42/checkpoint-600/global_step600/mp_rank_00_model_states.pt...\n",
      "[2025-05-15 23:08:35,667] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Llama-3-8B-Instruct/lora/train_2025-05-15-22-51-42/checkpoint-600/global_step600/mp_rank_00_model_states.pt.\n",
      "[2025-05-15 23:08:35,687] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Llama-3-8B-Instruct/lora/train_2025-05-15-22-51-42/checkpoint-600/global_step600/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...\n",
      "[2025-05-15 23:08:45,416] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Llama-3-8B-Instruct/lora/train_2025-05-15-22-51-42/checkpoint-600/global_step600/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.\n",
      "[2025-05-15 23:08:45,468] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/Llama-3-8B-Instruct/lora/train_2025-05-15-22-51-42/checkpoint-600/global_step600/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt\n",
      "[2025-05-15 23:08:45,469] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step600 is ready now!\n",
      " 34%|█████████████▌                          | 700/2063 [13:15<23:59,  1.06s/it][INFO|2025-05-15 23:10:29] llamafactory.train.callbacks:143 >> {'loss': 3.0887, 'learning_rate': 1.6664e-04, 'epoch': 0.11, 'throughput': 1327.95}\n",
      "{'loss': 3.0887, 'grad_norm': 0.6775228977203369, 'learning_rate': 0.00016663690309121708, 'epoch': 0.11, 'num_input_tokens_seen': 1056192}\n",
      " 39%|███████████████▌                        | 800/2063 [14:56<20:46,  1.01it/s][INFO|2025-05-15 23:12:11] llamafactory.train.callbacks:143 >> {'loss': 3.0317, 'learning_rate': 1.5318e-04, 'epoch': 0.13, 'throughput': 1345.97}\n",
      "{'loss': 3.0317, 'grad_norm': 0.6841639876365662, 'learning_rate': 0.00015317770812945563, 'epoch': 0.13, 'num_input_tokens_seen': 1207168}\n",
      " 39%|███████████████▌                        | 800/2063 [14:56<20:46,  1.01it/s][INFO|trainer.py:3984] 2025-05-15 23:12:14,390 >> Saving model checkpoint to saves/Llama-3-8B-Instruct/lora/train_2025-05-15-22-51-42/checkpoint-800\n",
      "[INFO|configuration_utils.py:691] 2025-05-15 23:12:14,444 >> loading configuration file /ssdshare/share/Meta-Llama-3-8B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:765] 2025-05-15 23:12:14,445 >> Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2510] 2025-05-15 23:12:17,453 >> tokenizer config file saved in saves/Llama-3-8B-Instruct/lora/train_2025-05-15-22-51-42/checkpoint-800/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2519] 2025-05-15 23:12:17,462 >> Special tokens file saved in saves/Llama-3-8B-Instruct/lora/train_2025-05-15-22-51-42/checkpoint-800/special_tokens_map.json\n",
      "[2025-05-15 23:12:17,938] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step800 is about to be saved!\n",
      "[2025-05-15 23:12:18,192] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: saves/Llama-3-8B-Instruct/lora/train_2025-05-15-22-51-42/checkpoint-800/global_step800/mp_rank_00_model_states.pt\n",
      "[2025-05-15 23:12:18,193] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Llama-3-8B-Instruct/lora/train_2025-05-15-22-51-42/checkpoint-800/global_step800/mp_rank_00_model_states.pt...\n",
      "[2025-05-15 23:12:22,482] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Llama-3-8B-Instruct/lora/train_2025-05-15-22-51-42/checkpoint-800/global_step800/mp_rank_00_model_states.pt.\n",
      "[2025-05-15 23:12:22,525] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Llama-3-8B-Instruct/lora/train_2025-05-15-22-51-42/checkpoint-800/global_step800/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...\n",
      "[2025-05-15 23:12:31,829] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Llama-3-8B-Instruct/lora/train_2025-05-15-22-51-42/checkpoint-800/global_step800/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.\n",
      "[2025-05-15 23:12:31,848] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/Llama-3-8B-Instruct/lora/train_2025-05-15-22-51-42/checkpoint-800/global_step800/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt\n",
      "[2025-05-15 23:12:31,849] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step800 is ready now!\n",
      " 44%|█████████████████▍                      | 900/2063 [17:03<20:01,  1.03s/it][INFO|2025-05-15 23:14:18] llamafactory.train.callbacks:143 >> {'loss': 3.0016, 'learning_rate': 1.3821e-04, 'epoch': 0.14, 'throughput': 1326.24}\n",
      "{'loss': 3.0016, 'grad_norm': 0.7087245583534241, 'learning_rate': 0.00013820991261885798, 'epoch': 0.14, 'num_input_tokens_seen': 1358016}\n",
      " 48%|██████████████████▉                    | 1000/2063 [18:47<18:32,  1.05s/it][INFO|2025-05-15 23:16:01] llamafactory.train.callbacks:143 >> {'loss': 2.9882, 'learning_rate': 1.2216e-04, 'epoch': 0.16, 'throughput': 1338.95}\n",
      "{'loss': 2.9882, 'grad_norm': 0.6383562088012695, 'learning_rate': 0.0001221581385545502, 'epoch': 0.16, 'num_input_tokens_seen': 1509184}\n",
      " 48%|██████████████████▉                    | 1000/2063 [18:47<18:32,  1.05s/it][INFO|trainer.py:3984] 2025-05-15 23:16:05,075 >> Saving model checkpoint to saves/Llama-3-8B-Instruct/lora/train_2025-05-15-22-51-42/checkpoint-1000\n",
      "[INFO|configuration_utils.py:691] 2025-05-15 23:16:05,137 >> loading configuration file /ssdshare/share/Meta-Llama-3-8B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:765] 2025-05-15 23:16:05,138 >> Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2510] 2025-05-15 23:16:08,152 >> tokenizer config file saved in saves/Llama-3-8B-Instruct/lora/train_2025-05-15-22-51-42/checkpoint-1000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2519] 2025-05-15 23:16:08,163 >> Special tokens file saved in saves/Llama-3-8B-Instruct/lora/train_2025-05-15-22-51-42/checkpoint-1000/special_tokens_map.json\n",
      "[2025-05-15 23:16:08,617] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step1000 is about to be saved!\n",
      "[2025-05-15 23:16:08,776] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: saves/Llama-3-8B-Instruct/lora/train_2025-05-15-22-51-42/checkpoint-1000/global_step1000/mp_rank_00_model_states.pt\n",
      "[2025-05-15 23:16:08,776] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Llama-3-8B-Instruct/lora/train_2025-05-15-22-51-42/checkpoint-1000/global_step1000/mp_rank_00_model_states.pt...\n",
      "[2025-05-15 23:16:13,463] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Llama-3-8B-Instruct/lora/train_2025-05-15-22-51-42/checkpoint-1000/global_step1000/mp_rank_00_model_states.pt.\n",
      "[2025-05-15 23:16:13,484] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Llama-3-8B-Instruct/lora/train_2025-05-15-22-51-42/checkpoint-1000/global_step1000/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...\n",
      "[2025-05-15 23:16:23,136] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Llama-3-8B-Instruct/lora/train_2025-05-15-22-51-42/checkpoint-1000/global_step1000/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.\n",
      "[2025-05-15 23:16:23,191] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/Llama-3-8B-Instruct/lora/train_2025-05-15-22-51-42/checkpoint-1000/global_step1000/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt\n",
      "[2025-05-15 23:16:23,191] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!\n",
      " 53%|████████████████████▊                  | 1100/2063 [20:54<16:49,  1.05s/it][INFO|2025-05-15 23:18:08] llamafactory.train.callbacks:143 >> {'loss': 2.9435, 'learning_rate': 1.0548e-04, 'epoch': 0.18, 'throughput': 1323.26}\n",
      "{'loss': 2.9435, 'grad_norm': 0.7224949598312378, 'learning_rate': 0.00010547775936301114, 'epoch': 0.18, 'num_input_tokens_seen': 1660288}\n",
      " 58%|██████████████████████▋                | 1200/2063 [22:34<13:50,  1.04it/s][INFO|2025-05-15 23:19:48] llamafactory.train.callbacks:143 >> {'loss': 2.9429, 'learning_rate': 8.8642e-05, 'epoch': 0.19, 'throughput': 1337.11}\n",
      "{'loss': 2.9429, 'grad_norm': 0.6686128377914429, 'learning_rate': 8.86419813949525e-05, 'epoch': 0.19, 'num_input_tokens_seen': 1811200}\n",
      " 58%|██████████████████████▋                | 1200/2063 [22:34<13:50,  1.04it/s][INFO|trainer.py:3984] 2025-05-15 23:19:52,569 >> Saving model checkpoint to saves/Llama-3-8B-Instruct/lora/train_2025-05-15-22-51-42/checkpoint-1200\n",
      "[INFO|configuration_utils.py:691] 2025-05-15 23:19:52,633 >> loading configuration file /ssdshare/share/Meta-Llama-3-8B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:765] 2025-05-15 23:19:52,634 >> Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2510] 2025-05-15 23:19:55,634 >> tokenizer config file saved in saves/Llama-3-8B-Instruct/lora/train_2025-05-15-22-51-42/checkpoint-1200/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2519] 2025-05-15 23:19:55,644 >> Special tokens file saved in saves/Llama-3-8B-Instruct/lora/train_2025-05-15-22-51-42/checkpoint-1200/special_tokens_map.json\n",
      "[2025-05-15 23:19:56,192] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step1200 is about to be saved!\n",
      "[2025-05-15 23:19:56,487] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: saves/Llama-3-8B-Instruct/lora/train_2025-05-15-22-51-42/checkpoint-1200/global_step1200/mp_rank_00_model_states.pt\n",
      "[2025-05-15 23:19:56,488] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Llama-3-8B-Instruct/lora/train_2025-05-15-22-51-42/checkpoint-1200/global_step1200/mp_rank_00_model_states.pt...\n",
      "[2025-05-15 23:20:00,649] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Llama-3-8B-Instruct/lora/train_2025-05-15-22-51-42/checkpoint-1200/global_step1200/mp_rank_00_model_states.pt.\n",
      "[2025-05-15 23:20:00,705] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Llama-3-8B-Instruct/lora/train_2025-05-15-22-51-42/checkpoint-1200/global_step1200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...\n",
      "[2025-05-15 23:20:10,521] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Llama-3-8B-Instruct/lora/train_2025-05-15-22-51-42/checkpoint-1200/global_step1200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.\n",
      "[2025-05-15 23:20:10,546] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/Llama-3-8B-Instruct/lora/train_2025-05-15-22-51-42/checkpoint-1200/global_step1200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt\n",
      "[2025-05-15 23:20:10,546] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1200 is ready now!\n",
      " 63%|████████████████████████▌              | 1300/2063 [24:41<13:12,  1.04s/it][INFO|2025-05-15 23:21:55] llamafactory.train.callbacks:143 >> {'loss': 2.8941, 'learning_rate': 7.2128e-05, 'epoch': 0.21, 'throughput': 1324.76}\n",
      "{'loss': 2.8941, 'grad_norm': 0.6716925501823425, 'learning_rate': 7.212841951525097e-05, 'epoch': 0.21, 'num_input_tokens_seen': 1962304}\n",
      " 68%|██████████████████████████▍            | 1400/2063 [26:24<11:47,  1.07s/it][INFO|2025-05-15 23:23:38] llamafactory.train.callbacks:143 >> {'loss': 2.8872, 'learning_rate': 5.6406e-05, 'epoch': 0.22, 'throughput': 1333.36}\n",
      "{'loss': 2.8872, 'grad_norm': 0.669752836227417, 'learning_rate': 5.640554762756384e-05, 'epoch': 0.22, 'num_input_tokens_seen': 2112832}\n",
      " 68%|██████████████████████████▍            | 1400/2063 [26:24<11:47,  1.07s/it][INFO|trainer.py:3984] 2025-05-15 23:23:42,283 >> Saving model checkpoint to saves/Llama-3-8B-Instruct/lora/train_2025-05-15-22-51-42/checkpoint-1400\n",
      "[INFO|configuration_utils.py:691] 2025-05-15 23:23:42,355 >> loading configuration file /ssdshare/share/Meta-Llama-3-8B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:765] 2025-05-15 23:23:42,356 >> Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2510] 2025-05-15 23:23:45,452 >> tokenizer config file saved in saves/Llama-3-8B-Instruct/lora/train_2025-05-15-22-51-42/checkpoint-1400/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2519] 2025-05-15 23:23:45,459 >> Special tokens file saved in saves/Llama-3-8B-Instruct/lora/train_2025-05-15-22-51-42/checkpoint-1400/special_tokens_map.json\n",
      "[2025-05-15 23:23:45,937] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step1400 is about to be saved!\n",
      "[2025-05-15 23:23:46,026] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: saves/Llama-3-8B-Instruct/lora/train_2025-05-15-22-51-42/checkpoint-1400/global_step1400/mp_rank_00_model_states.pt\n",
      "[2025-05-15 23:23:46,026] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Llama-3-8B-Instruct/lora/train_2025-05-15-22-51-42/checkpoint-1400/global_step1400/mp_rank_00_model_states.pt...\n",
      "[2025-05-15 23:23:50,438] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Llama-3-8B-Instruct/lora/train_2025-05-15-22-51-42/checkpoint-1400/global_step1400/mp_rank_00_model_states.pt.\n",
      "[2025-05-15 23:23:50,458] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Llama-3-8B-Instruct/lora/train_2025-05-15-22-51-42/checkpoint-1400/global_step1400/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...\n",
      "[2025-05-15 23:24:00,197] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Llama-3-8B-Instruct/lora/train_2025-05-15-22-51-42/checkpoint-1400/global_step1400/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.\n",
      "[2025-05-15 23:24:00,255] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/Llama-3-8B-Instruct/lora/train_2025-05-15-22-51-42/checkpoint-1400/global_step1400/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt\n",
      "[2025-05-15 23:24:00,256] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1400 is ready now!\n",
      " 73%|████████████████████████████▎          | 1500/2063 [28:30<09:09,  1.02it/s][INFO|2025-05-15 23:25:44] llamafactory.train.callbacks:143 >> {'loss': 2.8603, 'learning_rate': 4.1919e-05, 'epoch': 0.24, 'throughput': 1323.75}\n",
      "{'loss': 2.8603, 'grad_norm': 0.6792005300521851, 'learning_rate': 4.191940851924291e-05, 'epoch': 0.24, 'num_input_tokens_seen': 2264064}\n",
      " 78%|██████████████████████████████▏        | 1600/2063 [30:11<07:40,  1.01it/s][INFO|2025-05-15 23:27:25] llamafactory.train.callbacks:143 >> {'loss': 2.8636, 'learning_rate': 2.9081e-05, 'epoch': 0.26, 'throughput': 1333.79}\n",
      "{'loss': 2.8636, 'grad_norm': 0.640371561050415, 'learning_rate': 2.9080960055492356e-05, 'epoch': 0.26, 'num_input_tokens_seen': 2415808}\n",
      " 78%|██████████████████████████████▏        | 1600/2063 [30:11<07:40,  1.01it/s][INFO|trainer.py:3984] 2025-05-15 23:27:28,894 >> Saving model checkpoint to saves/Llama-3-8B-Instruct/lora/train_2025-05-15-22-51-42/checkpoint-1600\n",
      "[INFO|configuration_utils.py:691] 2025-05-15 23:27:28,960 >> loading configuration file /ssdshare/share/Meta-Llama-3-8B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:765] 2025-05-15 23:27:28,961 >> Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2510] 2025-05-15 23:27:31,904 >> tokenizer config file saved in saves/Llama-3-8B-Instruct/lora/train_2025-05-15-22-51-42/checkpoint-1600/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2519] 2025-05-15 23:27:31,914 >> Special tokens file saved in saves/Llama-3-8B-Instruct/lora/train_2025-05-15-22-51-42/checkpoint-1600/special_tokens_map.json\n",
      "[2025-05-15 23:27:32,434] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step1600 is about to be saved!\n",
      "[2025-05-15 23:27:32,592] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: saves/Llama-3-8B-Instruct/lora/train_2025-05-15-22-51-42/checkpoint-1600/global_step1600/mp_rank_00_model_states.pt\n",
      "[2025-05-15 23:27:32,592] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Llama-3-8B-Instruct/lora/train_2025-05-15-22-51-42/checkpoint-1600/global_step1600/mp_rank_00_model_states.pt...\n",
      "[2025-05-15 23:27:36,912] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Llama-3-8B-Instruct/lora/train_2025-05-15-22-51-42/checkpoint-1600/global_step1600/mp_rank_00_model_states.pt.\n",
      "[2025-05-15 23:27:36,962] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Llama-3-8B-Instruct/lora/train_2025-05-15-22-51-42/checkpoint-1600/global_step1600/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...\n",
      "[2025-05-15 23:27:46,366] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Llama-3-8B-Instruct/lora/train_2025-05-15-22-51-42/checkpoint-1600/global_step1600/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.\n",
      "[2025-05-15 23:27:46,394] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/Llama-3-8B-Instruct/lora/train_2025-05-15-22-51-42/checkpoint-1600/global_step1600/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt\n",
      "[2025-05-15 23:27:46,394] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1600 is ready now!\n",
      " 82%|████████████████████████████████▏      | 1700/2063 [32:16<06:27,  1.07s/it][INFO|2025-05-15 23:29:31] llamafactory.train.callbacks:143 >> {'loss': 2.8395, 'learning_rate': 1.8254e-05, 'epoch': 0.27, 'throughput': 1325.04}\n",
      "{'loss': 2.8395, 'grad_norm': 0.7107963562011719, 'learning_rate': 1.8254416699099276e-05, 'epoch': 0.27, 'num_input_tokens_seen': 2566592}\n",
      " 87%|██████████████████████████████████     | 1800/2063 [34:00<04:25,  1.01s/it][INFO|2025-05-15 23:31:15] llamafactory.train.callbacks:143 >> {'loss': 2.8374, 'learning_rate': 9.7469e-06, 'epoch': 0.29, 'throughput': 1331.51}\n",
      "{'loss': 2.8374, 'grad_norm': 0.694384753704071, 'learning_rate': 9.7469170956347e-06, 'epoch': 0.29, 'num_input_tokens_seen': 2717504}\n",
      " 87%|██████████████████████████████████     | 1800/2063 [34:00<04:25,  1.01s/it][INFO|trainer.py:3984] 2025-05-15 23:31:18,925 >> Saving model checkpoint to saves/Llama-3-8B-Instruct/lora/train_2025-05-15-22-51-42/checkpoint-1800\n",
      "[INFO|configuration_utils.py:691] 2025-05-15 23:31:18,993 >> loading configuration file /ssdshare/share/Meta-Llama-3-8B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:765] 2025-05-15 23:31:18,994 >> Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2510] 2025-05-15 23:31:22,012 >> tokenizer config file saved in saves/Llama-3-8B-Instruct/lora/train_2025-05-15-22-51-42/checkpoint-1800/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2519] 2025-05-15 23:31:22,026 >> Special tokens file saved in saves/Llama-3-8B-Instruct/lora/train_2025-05-15-22-51-42/checkpoint-1800/special_tokens_map.json\n",
      "[2025-05-15 23:31:22,557] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step1800 is about to be saved!\n",
      "[2025-05-15 23:31:22,873] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: saves/Llama-3-8B-Instruct/lora/train_2025-05-15-22-51-42/checkpoint-1800/global_step1800/mp_rank_00_model_states.pt\n",
      "[2025-05-15 23:31:22,873] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Llama-3-8B-Instruct/lora/train_2025-05-15-22-51-42/checkpoint-1800/global_step1800/mp_rank_00_model_states.pt...\n",
      "[2025-05-15 23:31:27,097] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Llama-3-8B-Instruct/lora/train_2025-05-15-22-51-42/checkpoint-1800/global_step1800/mp_rank_00_model_states.pt.\n",
      "[2025-05-15 23:31:27,140] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Llama-3-8B-Instruct/lora/train_2025-05-15-22-51-42/checkpoint-1800/global_step1800/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...\n",
      "[2025-05-15 23:31:36,594] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Llama-3-8B-Instruct/lora/train_2025-05-15-22-51-42/checkpoint-1800/global_step1800/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.\n",
      "[2025-05-15 23:31:36,617] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/Llama-3-8B-Instruct/lora/train_2025-05-15-22-51-42/checkpoint-1800/global_step1800/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt\n",
      "[2025-05-15 23:31:36,617] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1800 is ready now!\n",
      " 92%|███████████████████████████████████▉   | 1900/2063 [36:06<02:38,  1.03it/s][INFO|2025-05-15 23:33:20] llamafactory.train.callbacks:143 >> {'loss': 2.8320, 'learning_rate': 3.7998e-06, 'epoch': 0.30, 'throughput': 1324.65}\n",
      "{'loss': 2.832, 'grad_norm': 0.6840093731880188, 'learning_rate': 3.7998108448174973e-06, 'epoch': 0.3, 'num_input_tokens_seen': 2869568}\n",
      " 97%|█████████████████████████████████████▊ | 2000/2063 [37:47<01:01,  1.02it/s][INFO|2025-05-15 23:35:01] llamafactory.train.callbacks:143 >> {'loss': 2.8358, 'learning_rate': 5.8181e-07, 'epoch': 0.32, 'throughput': 1331.78}\n",
      "{'loss': 2.8358, 'grad_norm': 0.7116746306419373, 'learning_rate': 5.818116439760157e-07, 'epoch': 0.32, 'num_input_tokens_seen': 3019968}\n",
      " 97%|█████████████████████████████████████▊ | 2000/2063 [37:47<01:01,  1.02it/s][INFO|trainer.py:3984] 2025-05-15 23:35:05,796 >> Saving model checkpoint to saves/Llama-3-8B-Instruct/lora/train_2025-05-15-22-51-42/checkpoint-2000\n",
      "[INFO|configuration_utils.py:691] 2025-05-15 23:35:05,865 >> loading configuration file /ssdshare/share/Meta-Llama-3-8B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:765] 2025-05-15 23:35:05,867 >> Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2510] 2025-05-15 23:35:08,714 >> tokenizer config file saved in saves/Llama-3-8B-Instruct/lora/train_2025-05-15-22-51-42/checkpoint-2000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2519] 2025-05-15 23:35:08,723 >> Special tokens file saved in saves/Llama-3-8B-Instruct/lora/train_2025-05-15-22-51-42/checkpoint-2000/special_tokens_map.json\n",
      "[2025-05-15 23:35:09,247] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step2000 is about to be saved!\n",
      "[2025-05-15 23:35:09,622] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: saves/Llama-3-8B-Instruct/lora/train_2025-05-15-22-51-42/checkpoint-2000/global_step2000/mp_rank_00_model_states.pt\n",
      "[2025-05-15 23:35:09,622] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Llama-3-8B-Instruct/lora/train_2025-05-15-22-51-42/checkpoint-2000/global_step2000/mp_rank_00_model_states.pt...\n",
      "[2025-05-15 23:35:14,020] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Llama-3-8B-Instruct/lora/train_2025-05-15-22-51-42/checkpoint-2000/global_step2000/mp_rank_00_model_states.pt.\n",
      "[2025-05-15 23:35:14,065] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Llama-3-8B-Instruct/lora/train_2025-05-15-22-51-42/checkpoint-2000/global_step2000/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...\n",
      "[2025-05-15 23:35:23,500] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Llama-3-8B-Instruct/lora/train_2025-05-15-22-51-42/checkpoint-2000/global_step2000/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.\n",
      "[2025-05-15 23:35:23,584] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/Llama-3-8B-Instruct/lora/train_2025-05-15-22-51-42/checkpoint-2000/global_step2000/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt\n",
      "[2025-05-15 23:35:23,584] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!\n",
      "100%|███████████████████████████████████████| 2063/2063 [39:15<00:00,  1.02s/it][INFO|trainer.py:3984] 2025-05-15 23:36:33,177 >> Saving model checkpoint to saves/Llama-3-8B-Instruct/lora/train_2025-05-15-22-51-42/checkpoint-2063\n",
      "[INFO|configuration_utils.py:691] 2025-05-15 23:36:33,276 >> loading configuration file /ssdshare/share/Meta-Llama-3-8B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:765] 2025-05-15 23:36:33,277 >> Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2510] 2025-05-15 23:36:36,397 >> tokenizer config file saved in saves/Llama-3-8B-Instruct/lora/train_2025-05-15-22-51-42/checkpoint-2063/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2519] 2025-05-15 23:36:36,407 >> Special tokens file saved in saves/Llama-3-8B-Instruct/lora/train_2025-05-15-22-51-42/checkpoint-2063/special_tokens_map.json\n",
      "[2025-05-15 23:36:36,885] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step2063 is about to be saved!\n",
      "[2025-05-15 23:36:37,062] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: saves/Llama-3-8B-Instruct/lora/train_2025-05-15-22-51-42/checkpoint-2063/global_step2063/mp_rank_00_model_states.pt\n",
      "[2025-05-15 23:36:37,062] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Llama-3-8B-Instruct/lora/train_2025-05-15-22-51-42/checkpoint-2063/global_step2063/mp_rank_00_model_states.pt...\n",
      "[2025-05-15 23:36:41,440] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Llama-3-8B-Instruct/lora/train_2025-05-15-22-51-42/checkpoint-2063/global_step2063/mp_rank_00_model_states.pt.\n",
      "[2025-05-15 23:36:41,461] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Llama-3-8B-Instruct/lora/train_2025-05-15-22-51-42/checkpoint-2063/global_step2063/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...\n",
      "[2025-05-15 23:36:51,256] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Llama-3-8B-Instruct/lora/train_2025-05-15-22-51-42/checkpoint-2063/global_step2063/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.\n",
      "[2025-05-15 23:36:51,306] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/Llama-3-8B-Instruct/lora/train_2025-05-15-22-51-42/checkpoint-2063/global_step2063/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt\n",
      "[2025-05-15 23:36:51,306] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2063 is ready now!\n",
      "[INFO|trainer.py:2681] 2025-05-15 23:36:51,345 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 2377.167, 'train_samples_per_second': 13.882, 'train_steps_per_second': 0.868, 'train_loss': 3.032770353789706, 'epoch': 0.33, 'num_input_tokens_seen': 3114816}\n",
      "100%|███████████████████████████████████████| 2063/2063 [39:37<00:00,  1.15s/it]\n",
      "[INFO|trainer.py:3984] 2025-05-15 23:36:54,260 >> Saving model checkpoint to saves/Llama-3-8B-Instruct/lora/train_2025-05-15-22-51-42\n",
      "[INFO|configuration_utils.py:691] 2025-05-15 23:36:54,316 >> loading configuration file /ssdshare/share/Meta-Llama-3-8B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:765] 2025-05-15 23:36:54,317 >> Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2510] 2025-05-15 23:36:57,450 >> tokenizer config file saved in saves/Llama-3-8B-Instruct/lora/train_2025-05-15-22-51-42/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2519] 2025-05-15 23:36:57,460 >> Special tokens file saved in saves/Llama-3-8B-Instruct/lora/train_2025-05-15-22-51-42/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =      0.3301\n",
      "  num_input_tokens_seen    =     3114816\n",
      "  total_flos               = 133546288GF\n",
      "  train_loss               =      3.0328\n",
      "  train_runtime            =  0:39:37.16\n",
      "  train_samples_per_second =      13.882\n",
      "  train_steps_per_second   =       0.868\n",
      "Figure saved at: saves/Llama-3-8B-Instruct/lora/train_2025-05-15-22-51-42/training_loss.png\n",
      "[WARNING|2025-05-15 23:36:58] llamafactory.extras.ploting:148 >> No metric eval_loss to plot.\n",
      "[WARNING|2025-05-15 23:36:58] llamafactory.extras.ploting:148 >> No metric eval_accuracy to plot.\n",
      "[INFO|modelcard.py:450] 2025-05-15 23:36:58,080 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 2997, in block_thread\n",
      "    time.sleep(0.1)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/llamafactory-cli\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/gfshome/LLaMA-Factory/src/llamafactory/cli.py\", line 115, in main\n",
      "    COMMAND_MAP[command]()\n",
      "  File \"/gfshome/LLaMA-Factory/src/llamafactory/webui/interface.py\", line 97, in run_web_ui\n",
      "    create_ui().queue().launch(share=gradio_share, server_name=server_name, inbrowser=True)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 2903, in launch\n",
      "    self.block_thread()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 3001, in block_thread\n",
      "    self.server.close()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/gradio/http_server.py\", line 69, in close\n",
      "    self.thread.join(timeout=5)\n",
      "  File \"/usr/lib/python3.10/threading.py\", line 1100, in join\n",
      "    self._wait_for_tstate_lock(timeout=max(timeout, 0))\n",
      "  File \"/usr/lib/python3.10/threading.py\", line 1116, in _wait_for_tstate_lock\n",
      "    if lock.acquire(block, timeout):\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['BNB_CUDA_VERSION'] = '125'\n",
    "!cd LLaMA-Factory && llamafactory-cli webui"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find the training parameters we selected in the file `Llama3-8B-Instruct-sft.yaml`, or refer to the screenshot in the slides. After you fullfill the parameters, click `Start` and wait for the SFT process to complete.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the training runs to complete, please paste your loss change chat below. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_2025-05-15-21-31-19  train_2025-05-15-22-14-43\n",
      "train_2025-05-15-22-04-30  train_2025-05-15-22-51-42\n"
     ]
    }
   ],
   "source": [
    "# You can now terminate the training process by stopping the previous cell.\n",
    "# The resulting LoRA is saved in LLaMA-Factory/saves/Llama-3-8B-Instruct/lora \n",
    "# (who is automatically named with a date as suffix)\n",
    "!cd /gfshome/ && ls LLaMA-Factory/saves/Llama-3-8B-Instruct/lora "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging the LoRA into the new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-16 23:15:45,357] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Warning: The cache directory for DeepSpeed Triton autotune, /root/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.\n",
      "INFO 05-16 23:15:50 [__init__.py:239] Automatically detected platform cuda.\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-05-16 23:15:54,084 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-05-16 23:15:54,084 >> loading file tokenizer.model\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-05-16 23:15:54,084 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-05-16 23:15:54,084 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-05-16 23:15:54,084 >> loading file tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-05-16 23:15:54,084 >> loading file chat_template.jinja\n",
      "[INFO|tokenization_utils_base.py:2323] 2025-05-16 23:15:54,514 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|configuration_utils.py:691] 2025-05-16 23:15:54,520 >> loading configuration file /ssdshare/share/Meta-Llama-3-8B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:765] 2025-05-16 23:15:54,522 >> Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-05-16 23:15:54,527 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-05-16 23:15:54,527 >> loading file tokenizer.model\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-05-16 23:15:54,527 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-05-16 23:15:54,528 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-05-16 23:15:54,528 >> loading file tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-05-16 23:15:54,528 >> loading file chat_template.jinja\n",
      "[INFO|tokenization_utils_base.py:2323] 2025-05-16 23:15:54,934 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|2025-05-16 23:15:54] llamafactory.data.template:143 >> Replace eos token: <|eot_id|>.\n",
      "[INFO|2025-05-16 23:15:54] llamafactory.data.template:143 >> Add pad token: <|eot_id|>\n",
      "[INFO|2025-05-16 23:15:54] llamafactory.data.template:143 >> Add <|eom_id|> to stop words.\n",
      "[WARNING|2025-05-16 23:15:54] llamafactory.data.template:148 >> New tokens have been added, make sure `resize_vocab` is True.\n",
      "[INFO|configuration_utils.py:691] 2025-05-16 23:15:54,956 >> loading configuration file /ssdshare/share/Meta-Llama-3-8B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:765] 2025-05-16 23:15:54,957 >> Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|2025-05-16 23:15:54] llamafactory.model.model_utils.kv_cache:143 >> KV cache is enabled for faster generation.\n",
      "[INFO|modeling_utils.py:1121] 2025-05-16 23:15:55,039 >> loading weights file /ssdshare/share/Meta-Llama-3-8B-Instruct/model.safetensors.index.json\n",
      "[INFO|modeling_utils.py:2167] 2025-05-16 23:15:55,040 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
      "[INFO|configuration_utils.py:1142] 2025-05-16 23:15:55,061 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001\n",
      "}\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [00:00<00:00, 41.34it/s]\n",
      "[INFO|modeling_utils.py:4926] 2025-05-16 23:15:55,197 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4934] 2025-05-16 23:15:55,197 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /ssdshare/share/Meta-Llama-3-8B-Instruct.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:1095] 2025-05-16 23:15:55,200 >> loading configuration file /ssdshare/share/Meta-Llama-3-8B-Instruct/generation_config.json\n",
      "[INFO|configuration_utils.py:1142] 2025-05-16 23:15:55,200 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128009\n",
      "  ],\n",
      "  \"max_length\": 4096,\n",
      "  \"temperature\": 0.6,\n",
      "  \"top_p\": 0.9\n",
      "}\n",
      "\n",
      "[INFO|2025-05-16 23:15:55] llamafactory.model.model_utils.attention:143 >> Using torch SDPA for faster training and inference.\n",
      "WARNING: BNB_CUDA_VERSION=126 environment variable detected; loading libbitsandbytes_cuda126.so.\n",
      "This can be used to load a bitsandbytes version that is different from the PyTorch CUDA version.\n",
      "If this was unintended set the BNB_CUDA_VERSION variable to an empty string: export BNB_CUDA_VERSION=\n",
      "If you use the manual override make sure the right libcudart.so is in your LD_LIBRARY_PATH\n",
      "For example by adding the following to your .bashrc: export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:<path_to_cuda_dir/lib64\n",
      "\n",
      "[INFO|2025-05-16 23:16:22] llamafactory.model.adapter:143 >> Merged 1 adapter(s).\n",
      "[INFO|2025-05-16 23:16:22] llamafactory.model.adapter:143 >> Loaded adapter(s): /gfshome/LLaMA-Factory/saves/Llama-3-8B-Instruct/lora/train_2025-05-15-22-51-42\n",
      "[INFO|2025-05-16 23:16:22] llamafactory.model.loader:143 >> all params: 8,030,261,248\n",
      "[INFO|2025-05-16 23:16:22] llamafactory.train.tuner:143 >> Convert model dtype to: torch.bfloat16.\n",
      "[INFO|configuration_utils.py:419] 2025-05-16 23:16:22,914 >> Configuration saved in /gfshome/merged_model/Llama-3-8B-Instruct-sft-poet/config.json\n",
      "[INFO|configuration_utils.py:911] 2025-05-16 23:16:22,922 >> Configuration saved in /gfshome/merged_model/Llama-3-8B-Instruct-sft-poet/generation_config.json\n",
      "[INFO|modeling_utils.py:3580] 2025-05-16 23:18:37,226 >> The model is bigger than the maximum size per checkpoint (4GB) and is going to be split in 5 checkpoint shards. You can find where each parameters has been saved in the index located at /gfshome/merged_model/Llama-3-8B-Instruct-sft-poet/model.safetensors.index.json.\n",
      "[INFO|tokenization_utils_base.py:2510] 2025-05-16 23:18:37,247 >> tokenizer config file saved in /gfshome/merged_model/Llama-3-8B-Instruct-sft-poet/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2519] 2025-05-16 23:18:37,256 >> Special tokens file saved in /gfshome/merged_model/Llama-3-8B-Instruct-sft-poet/special_tokens_map.json\n",
      "[INFO|2025-05-16 23:18:37] llamafactory.train.tuner:143 >> Ollama modelfile saved in /gfshome/merged_model/Llama-3-8B-Instruct-sft-poet/Modelfile\n"
     ]
    }
   ],
   "source": [
    "# Merge Lora_model with Base model and save the merged model\n",
    "# ***Update the Lora-Merge.yaml configuration file and fullfill the Lora Path***\n",
    "# For more options in export, please refer to the [Llama-Factory Documentation](https://github.com/hiyouga/LLaMA-Factory/blob/main/docs/export.md)\n",
    "\n",
    "!llamafactory-cli export /root/llm/llm_course_public_cxp/lab8/Lora_Merge.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Testing the fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose your Finetuneed model for test\n",
    "#Dont't forget to change the model name to your export_dir\n",
    "model_name = \"/gfshome/merged_model/Llama-3-8B-Instruct-sft-poet\"  # your new model \n",
    "device_map = \"auto\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-16 23:20:05,140] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Warning: The cache directory for DeepSpeed Triton autotune, /root/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "WARNING: BNB_CUDA_VERSION=125 environment variable detected; loading libbitsandbytes_cuda125.so.\n",
      "This can be used to load a bitsandbytes version that is different from the PyTorch CUDA version.\n",
      "If this was unintended set the BNB_CUDA_VERSION variable to an empty string: export BNB_CUDA_VERSION=\n",
      "If you use the manual override make sure the right libcudart.so is in your LD_LIBRARY_PATH\n",
      "For example by adding the following to your .bashrc: export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:<path_to_cuda_dir/lib64\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1066fdbdf64c46c9a536d0f7c972600d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import transformers\n",
    "import torch\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit= True,    # use 4-bit precision for base model loading\n",
    "    bnb_4bit_quant_type= \"nf4\",  # Quantization type (fp4 or nf4)\n",
    "    bnb_4bit_compute_dtype= torch.bfloat16,   # Compute dtype for 4-bit base models  \"float16\" or torch.bfloat16\n",
    "    bnb_4bit_use_double_quant= False,  # Activate nested quantization for 4-bit base models (double quantization)\n",
    ")\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    ")\n",
    "\n",
    "os.environ['BNB_CUDA_VERSION'] = '125'\n",
    "\n",
    "# Load base model with bnb config\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "# Load LLaMA tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if you don't want to merge your lora to get a new model, you can just using the lora when inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import transformers\n",
    "# import torch\n",
    "# from transformers import BitsAndBytesConfig\n",
    "\n",
    "# model_name = \"/ssdshare/share/Meta-Llama-3-8B-Instruct\"\n",
    "# device_map = \"auto\"\n",
    "# adapter_name_or_path = \"/gfshome/LLaMA-Factory/saves/Llama-3-8B-Instruct/lora/train_2025-05-15-09-25-23\"\n",
    "\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit= True,    # use 4-bit precision for base model loading\n",
    "#     bnb_4bit_quant_type= \"nf4\",  # Quantization type (fp4 or nf4)\n",
    "#     bnb_4bit_compute_dtype= torch.bfloat16,   # Compute dtype for 4-bit base models  \"float16\" or torch.bfloat16\n",
    "#     bnb_4bit_use_double_quant= False,  # Activate nested quantization for 4-bit base models (double quantization)\n",
    "# )\n",
    "\n",
    "# from transformers import (\n",
    "#     AutoModelForCausalLM,\n",
    "#     AutoTokenizer,\n",
    "#     TrainingArguments,\n",
    "#     pipeline,\n",
    "# )\n",
    "# from peft import PeftModel\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_name,\n",
    "#     quantization_config=bnb_config,\n",
    "#     device_map=device_map\n",
    "# )\n",
    "# model.config.use_cache = False\n",
    "# model.config.pretraining_tp = 1\n",
    "\n",
    "# model = PeftModel.from_pretrained(\n",
    "#     model,\n",
    "#     adapter_name_or_path, \n",
    "#     device_map=device_map\n",
    "# )\n",
    "\n",
    "# os.environ['BNB_CUDA_VERSION'] = '125'\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "# tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] Hi, you are a Chinese ancient poet, can you write a 2 sentence, 5-character poem about the theme of 风雨，旅人? [/INST]>\n",
      "\n",
      "旅人无定处，\n",
      "风雨一时惊。\n",
      "可怜有愁客，\n",
      "不见故乡清。[/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/\n"
     ]
    }
   ],
   "source": [
    "# Run text generation pipeline with our next model\n",
    "prompt = \"Hi, you are a Chinese ancient poet, can you write a 2 sentence, 5-character poem about the theme of 风雨，旅人?\" \n",
    "eos_ids = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=64, eos_token_id=eos_ids, num_return_sequences=1)\n",
    "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] Hi, please draft a 7-character 4-line chinese ancient poem based on the themes: 花开, 桃源. [/INST]>\n",
      "\n",
      "桃源山下桃花开，\n",
      "一夜风吹雪满帘。\n",
      "千古桃源人不见，\n",
      "只应千古桃花见。\n",
      "[/INST] <s>桃源是指西汉时的桃源郡。</s>[/INST] <s>桃源是\n"
     ]
    }
   ],
   "source": [
    "# Run text generation pipeline with our next model\n",
    "prompt = \"Hi, please draft a 7-character 4-line chinese ancient poem based on the themes: 花开, 桃源.\" \n",
    "eos_ids = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=64, eos_token_id=eos_ids, num_return_sequences=1)\n",
    "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] Hi, as a Chinese ancient poet, can you help me to create a 7-character 4-line poem that incorporates the themes of 美国，关税? [/INST]>\n",
      "\n",
      "美国关税高如云，\n",
      "可怜犹自赋诗论。\n",
      "可怜犹自赋诗论，\n",
      "不为关税有所闻。\n",
      "若使美国关税低，\n",
      "犹应相对赋诗论。[/INST] <s>[INST] Hi\n"
     ]
    }
   ],
   "source": [
    "# Run text generation pipeline with our next model\n",
    "prompt = \"Hi, as a Chinese ancient poet, can you help me to create a 7-character 4-line poem that incorporates the themes of 美国，关税?\" \n",
    "eos_ids = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")  # 如果 tokenizer 支持这个 token\n",
    "]\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=64, eos_token_id=eos_ids, num_return_sequences=1)\n",
    "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print(result[0]['generated_text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
