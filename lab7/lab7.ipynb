{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 7: LLM API server and Web interfaces\n",
    "\n",
    "In this lecture, you will learn how to serve modern large models on Linux servers with easy-to-use user interface. We will be using Python as our main programming language, and we do not require knowledge about front-end language such as Javascript or CSS."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Calling Web Service APIs\n",
    "\n",
    "In this experiment, we'll equip you with the basic knowledge and practical skills to start making powerful HTTP requests in Python. We'll cover GET and POST methods, and explore JSON data exchange. So, buckle up, let's code!\n",
    "\n",
    "First, we will need `requests` library. It should be installed by default in your Python environment, but if you don't have it, you can install it using pip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirrors.cernet.edu.cn/pypi/web/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.8.30)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install requests"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Basic `GET`\n",
    "\n",
    "GET retrieves information from a specific web address (URL). Parameters are passed either in the path itself or as a query parameter (after ? in the URL).\n",
    "\n",
    "Let's try the GET method to retrieve a random joke!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status code: 200\n",
      "\n",
      "--- Response Text ---\n",
      "{\"categories\":[],\"created_at\":\"2020-01-05 13:42:23.240175\",\"icon_url\":\"https://api.chucknorris.io/img/avatar/chuck-norris.png\",\"id\":\"dzsV9TzMTnCbckmmbzNJyA\",\"updated_at\":\"2020-01-05 13:42:23.240175\",\"url\":\"https://api.chucknorris.io/jokes/dzsV9TzMTnCbckmmbzNJyA\",\"value\":\"When nature calls Chuck Norris hangs up\"}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Target URL\n",
    "url = \"https://api.chucknorris.io/jokes/random\"\n",
    "\n",
    "# Send a GET request and store the response\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check the response status code (2XX means success)\n",
    "print(f\"Status code: {response.status_code}\")\n",
    "\n",
    "# Access the response content (raw bytes)\n",
    "content = response.content\n",
    "\n",
    "# Decode the content to text (may differ depending on API)\n",
    "text = content.decode(response.encoding)\n",
    "\n",
    "# Print the response\n",
    "print(\"\\n--- Response Text ---\")\n",
    "print(text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Playing with JSON\n",
    "\n",
    "Many APIs and websites return data in the JSON format, a structured way to organize information. We can easily convert this JSON string to a Python dictionary for easy access:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'categories': [],\n",
      " 'created_at': '2020-01-05 13:42:23.240175',\n",
      " 'icon_url': 'https://api.chucknorris.io/img/avatar/chuck-norris.png',\n",
      " 'id': 'dzsV9TzMTnCbckmmbzNJyA',\n",
      " 'updated_at': '2020-01-05 13:42:23.240175',\n",
      " 'url': 'https://api.chucknorris.io/jokes/dzsV9TzMTnCbckmmbzNJyA',\n",
      " 'value': 'When nature calls Chuck Norris hangs up'}\n",
      "{\"categories\": [], \"created_at\": \"2020-01-05 13:42:23.240175\", \"icon_url\": \"https://api.chucknorris.io/img/avatar/chuck-norris.png\", \"id\": \"dzsV9TzMTnCbckmmbzNJyA\", \"updated_at\": \"2020-01-05 13:42:23.240175\", \"url\": \"https://api.chucknorris.io/jokes/dzsV9TzMTnCbckmmbzNJyA\", \"value\": \"When nature calls Chuck Norris hangs up\"}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "dict = json.loads(text)\n",
    "pprint(dict)\n",
    "\n",
    "encoded_json = json.dumps(dict)\n",
    "print(encoded_json)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Moving on to POST Requests\n",
    "\n",
    "While GET requests fetch data, POST requests send information to a server, like submitting a form. We'll be using a dummy API that echos the data we sent as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status code: 200\n",
      "{\n",
      "  \"args\": {}, \n",
      "  \"data\": \"\", \n",
      "  \"files\": {}, \n",
      "  \"form\": {\n",
      "    \"age\": \"30\", \n",
      "    \"name\": \"John Doe\"\n",
      "  }, \n",
      "  \"headers\": {\n",
      "    \"Accept\": \"*/*\", \n",
      "    \"Accept-Encoding\": \"gzip, deflate, zstd\", \n",
      "    \"Content-Length\": \"20\", \n",
      "    \"Content-Type\": \"application/x-www-form-urlencoded\", \n",
      "    \"Host\": \"httpbin.org\", \n",
      "    \"User-Agent\": \"python-requests/2.32.3\", \n",
      "    \"X-Amzn-Trace-Id\": \"Root=1-6825853f-5cfb7df74b800eb05eaccce2\"\n",
      "  }, \n",
      "  \"json\": null, \n",
      "  \"method\": \"POST\", \n",
      "  \"origin\": \"114.253.255.159\", \n",
      "  \"url\": \"https://httpbin.org/anything\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define URL and data\n",
    "url = \"https://httpbin.org/anything\"\n",
    "data = {\"name\": \"John Doe\", \"age\": 30}  # a python dictionary\n",
    "\n",
    "# Send POST request with data\n",
    "response = requests.post(url, data=data) # data is automatically encoded to json\n",
    "\n",
    "# Check status code and print response\n",
    "print(f\"Status code: {response.status_code}\")\n",
    "print(response.text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the sent data is actually received by the server (`form` shows the exactly the same data we sent).\n",
    "\n",
    "This is just the tip of the iceberg! Now you have seen how we can utilize the existing web service. In the remaining experiments, you will be building your own API server and web service with a nice user interface."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Creating an API server using FastAPI\n",
    "\n",
    "Most of you should have experienced the LLM APIs we provided, which allows your program accessing the power of large language models. Here we will guide you to build your own LLM service, using the `fastapi` library of Python.\n",
    "\n",
    "`fastapi` takes care of the job of launching a web server and serve the API calls. You only need to define a function that takes the input data from the request to produce output. `fastapi` will handle the rest things for you.\n",
    "\n",
    "First, install the dependency of `fastapi` if needed:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Basics on FastAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirrors.cernet.edu.cn/pypi/web/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: uvicorn in /usr/local/lib/python3.10/dist-packages (0.34.0)\n",
      "Requirement already satisfied: fastapi in /usr/local/lib/python3.10/dist-packages (0.115.9)\n",
      "Requirement already satisfied: websockets in /usr/local/lib/python3.10/dist-packages (15.0.1)\n",
      "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (8.1.7)\n",
      "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (0.14.0)\n",
      "Requirement already satisfied: typing-extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (4.12.2)\n",
      "Requirement already satisfied: starlette<0.46.0,>=0.40.0 in /usr/local/lib/python3.10/dist-packages (from fastapi) (0.45.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from fastapi) (2.9.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.23.4)\n",
      "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.10/dist-packages (from starlette<0.46.0,>=0.40.0->fastapi) (4.6.0)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.6.2->starlette<0.46.0,>=0.40.0->fastapi) (3.7)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.6.2->starlette<0.46.0,>=0.40.0->fastapi) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.6.2->starlette<0.46.0,>=0.40.0->fastapi) (1.2.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install uvicorn fastapi websockets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /tmp/fastapi_example.py\n"
     ]
    }
   ],
   "source": [
    "%%file /tmp/fastapi_example.py\n",
    "\n",
    "from fastapi import FastAPI, Request\n",
    "from pydantic import BaseModel\n",
    "import uvicorn\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "## path parameters\n",
    "@app.get('/g/{data}')\n",
    "async def process_data(data: str):\n",
    "    return f'Processed {data} by FastAPI!'\n",
    "\n",
    "fake_items_db = [{\"item_name\": \"Foo\"}, {\"item_name\": \"Bar\"}, {\"item_name\": \"Baz\"}]\n",
    "# Query parameters\n",
    "@app.get(\"/items/\")\n",
    "async def read_item(skip: int = 0, limit: int = 10):\n",
    "    return fake_items_db[skip : skip + limit]\n",
    "\n",
    "\n",
    "## The data model\n",
    "from typing import List\n",
    "class Sale(BaseModel):\n",
    "    day: int\n",
    "    price: float\n",
    "    \n",
    "class Item(BaseModel):\n",
    "    name: str\n",
    "    inventory: int | None = 10\n",
    "    sales: List[Sale] = []\n",
    "\n",
    "# Getting Parameters from Request\n",
    "@app.post(\"/post\")\n",
    "async def create_item(item: Item):\n",
    "    return f'Hello {item.name}, {item.inventory} in stock, sold {len(item.sales)} items'\n",
    "\n",
    "# The main() function is the entry point of the script\n",
    "if __name__ == '__main__':\n",
    "    uvicorn.run(app, host='0.0.0.0', port=54223, workers=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## run the following command in your terminal to start the server\n",
    "## python /tmp/fastapi_example.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status code: 200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "b'\"Processed hello by FastAPI!\"'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# you can visit your web service at:\n",
    "\n",
    "response = requests.get('http://localhost:54223/g/hello')\n",
    "print(f\"Status code: {response.status_code}\")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status code: 200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "b'[{\"item_name\":\"Baz\"}]'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using the query parameter\n",
    "response = requests.get('http://localhost:54223/items?skip=2&limit=3')\n",
    "print(f\"Status code: {response.status_code}\")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let the magic happen.  \n",
    "# Set port forwarding in your VSCode devcontainer to forward port 54223 to your local machine\n",
    "# Then visit `http://127.0.0.1:54223/g/hello` in your browser, you will be able to see the return string in the browser!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status code: 200\n",
      "\"Hello Apple, 33 in stock, sold 2 items\"\n"
     ]
    }
   ],
   "source": [
    "# Also test the POST processing, with a complex data structure as input\n",
    "\n",
    "url = \"http://localhost:54223/post\"\n",
    "data = { \"name\": \"Apple\", \n",
    "         \"inventory\": 33, \n",
    "         \"sales\": [{\"day\": 0, \"price\": 3.4}, {\"day\": 1, \"price\": 3.3}]\n",
    "         }\n",
    "encoded = json.dumps(data).encode(\"utf-8\")\n",
    "response = requests.post(url, data=encoded)  # the parameters should be encoded as JSON\n",
    "print(f\"Status code: {response.status_code}\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another FastAPI magic: automatic document generation\n",
    "# Visit http://localhost:54223/docs in your browser to see the API documentation\n",
    "# (Assuming that you have your port forwarding set up correctly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Creating an API to serve local LLM model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's recall how you run a local LLM.  The following scripts starts a Phi-4 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /tmp/local_llm.py\n"
     ]
    }
   ],
   "source": [
    "%%file /tmp/local_llm.py\n",
    "\n",
    "import os\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "\n",
    "def chat_resp(model, tokenizer, user_prompt=None, history=[]):\n",
    "    pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "    )   \n",
    "    generation_args = {\n",
    "        \"max_new_tokens\": 500,\n",
    "        \"return_full_text\": False,\n",
    "        \"temperature\": 0.6,\n",
    "        \"do_sample\": True,\n",
    "    }\n",
    "    if not history:\n",
    "        messages = [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},]\n",
    "    else:\n",
    "        messages = history\n",
    "    if user_prompt:\n",
    "        prompt_msg = [{\"role\": \"user\", \"content\": user_prompt}]\n",
    "        messages.extend(prompt_msg)\n",
    "    output = pipe(messages, **generation_args)\n",
    "    return output\n",
    "\n",
    "## The main function is the entry point of the script\n",
    "if __name__ == '__main__':\n",
    "    model_path = '/ssdshare/share/model/Phi-4-mini-instruct'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path, \n",
    "                                             device_map=\"cuda:0\", \n",
    "                                             torch_dtype=\"auto\", \n",
    "                                             trust_remote_code=True,\n",
    "                                             )\n",
    "    resp = chat_resp(model, tokenizer, \"What is the meaning of life?\")\n",
    "    print(resp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## first verify that you can run LLM locally correctly (it should print out the results, despite of lots of warnings.)\n",
    "## python /tmp/local_llm.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /tmp/llm_api.py\n"
     ]
    }
   ],
   "source": [
    "%%file /tmp/llm_api.py\n",
    "\n",
    "import os\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "         \n",
    "from fastapi import FastAPI, Request\n",
    "from pydantic import BaseModel\n",
    "import uvicorn\n",
    "\n",
    "from urllib.parse import unquote\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "def chat_resp(model, tokenizer, user_prompt=None, history=[]):\n",
    "    pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "    )   \n",
    "    generation_args = {\n",
    "        \"max_new_tokens\": 500,\n",
    "        \"return_full_text\": False,\n",
    "        \"temperature\": 0.6,\n",
    "        \"do_sample\": True,\n",
    "    }\n",
    "    if not history:\n",
    "        messages = [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},]\n",
    "    else:\n",
    "        messages = history\n",
    "    if user_prompt:\n",
    "        prompt_msg = [{\"role\": \"user\", \"content\": user_prompt}]\n",
    "        messages.extend(prompt_msg)\n",
    "    output = pipe(messages, **generation_args)\n",
    "    return output\n",
    "\n",
    "#### Your Task ####\n",
    "## Implement a GET handler that takes in a single string as prompt from user,\n",
    "## and return the response as a single string.\n",
    "@app.get('/run')\n",
    "async def run(q: str = \"\"):\n",
    "    prompt = unquote(q)  # Decode URL-encoded string\n",
    "    result = chat_resp(model, tokenizer, user_prompt=prompt)\n",
    "    # Extract the generated text from the result\n",
    "    return result[0][\"generated_text\"]\n",
    "#### End Task ####\n",
    "\n",
    "#### Your Task ####\n",
    "## Implement a POST handler that takes in a single string and a history\n",
    "## and return the response as a single string.\n",
    "class ChatRequest(BaseModel):\n",
    "    prompt: str\n",
    "    history: list = []\n",
    "\n",
    "@app.post('/chat')\n",
    "async def chat(request: ChatRequest):\n",
    "    result = chat_resp(model, tokenizer, user_prompt=request.prompt, history=request.history)\n",
    "    return result[0][\"generated_text\"]\n",
    "#### End Task ####\n",
    "\n",
    "#### Your Task ####\n",
    "## The main function is the entry point of the script, you should load the model\n",
    "## and then start the FastAPI server.\n",
    "if __name__ == '__main__':\n",
    "    model_path = '/ssdshare/share/model/Phi-4-mini-instruct'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path, \n",
    "                                           device_map=\"cuda:0\", \n",
    "                                           torch_dtype=\"auto\", \n",
    "                                           trust_remote_code=True,\n",
    "                                          )\n",
    "    uvicorn.run(app, host='0.0.0.0', port=54223, workers=1)\n",
    "#### End Task ####\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "## run the following command in your terminal to start the server\n",
    "## python /tmp/llm_api.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:54223/run?q=%E4%B8%AD%E5%9B%BD%E7%9A%84%E9%A6%96%E9%83%BD%E6%98%AF%E5%93%AA%E9%87%8C%EF%BC%9F\n",
      "Status code: 200\n",
      "\"中国的首都是北京。\"\n"
     ]
    }
   ],
   "source": [
    "## Run a single query to test the API, using GET\n",
    "\n",
    "import urllib.parse\n",
    "params = {\"q\": \"中国的首都是哪里？\"}\n",
    "prompt_url = urllib.parse.urlencode(params)\n",
    "url = f'http://localhost:54223/run?%s' % prompt_url\n",
    "print(url)\n",
    "response = requests.get(url)\n",
    "print(f\"Status code: {response.status_code}\")\n",
    "print(response.content.decode(response.encoding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: Hello, who are you?\n",
      "Assistant: Hello! I'm Phi, an AI developed by Microsoft. How can I help you today?\n",
      "--------------------------------------------------\n",
      "User: What's the capital of Japan?\n",
      "Assistant: The capital of Japan is Tokyo. It's not only the political center but also a major cultural and economic hub. If you have any more questions or need further assistance, feel free to ask!\n",
      "--------------------------------------------------\n",
      "User: Can you tell me more about its history?\n",
      "Assistant: Certainly! Tokyo, originally known as Edo, has a rich history that spans over 1,000 years. Here are some key points in its historical timeline:\n",
      "\n",
      "1. **Early History (Edo Period) (1603-1868)**:\n",
      "   - In 1603, Tokugawa Ieyasu established Edo as the capital of the Tokugawa shogunate, marking the beginning of the Edo period.\n",
      "   - Edo became the center of political, economic, and cultural life in Japan. It was a time of relative peace and stability, known as the Pax Tokugawa.\n",
      "   - The city developed into a vibrant urban center, surrounded by a massive wall and a series of moats. It became a bustling hub of trade, culture, and innovation.\n",
      "\n",
      "2. **Meiji Restoration (1868-1912)**:\n",
      "   - In 1868, Emperor Meiji was restored to the throne, and the capital moved from Edo to a new location, Kyoto, before eventually settling in Tokyo.\n",
      "   - The Meiji Restoration marked the end of the Tokugawa shogunate and the beginning of the Meiji era, characterized by rapid modernization and westernization.\n",
      "   - Tokyo became the political and cultural heart of Japan during this period, witnessing significant industrial and infrastructural development.\n",
      "\n",
      "3. **Modern Tokyo (Post-World War II)**:\n",
      "   - After World War II, Tokyo was heavily damaged but underwent rapid reconstruction and redevelopment.\n",
      "   - In 1943, Tokyo was renamed from its old name, Edo, to Shinjuku, but the name Tokyo was reinstated in 1951.\n",
      "   - Post-war Tokyo saw significant economic growth and became a global financial center. The city expanded rapidly, incorporating surrounding areas and becoming a sprawling metropolis.\n",
      "   - Tokyo has continued to be a major global hub for business, technology, culture, and entertainment, with landmarks such as the Tokyo Tower, Shibuya Crossing, and the Imperial Palace.\n",
      "\n",
      "Today, Tokyo is a dynamic and diverse city that blends traditional Japanese culture with cutting-edge technology and modern conveniences. Its history continues to shape its identity as one of the world's most influential and iconic cities. If you have any more questions or need further details, feel free to ask!\n",
      "--------------------------------------------------\n",
      "Full conversation history:\n",
      "User: Hello, who are you?\n",
      "Assistant: Hello! I'm Phi, an AI developed by Microsoft. How can I help you today?\n",
      "User: What's the capital of Japan?\n",
      "Assistant: The capital of Japan is Tokyo. It's not only the political center but also a major cultural and economic hub. If you have any more questions or need further assistance, feel free to ask!\n",
      "User: Can you tell me more about its history?\n",
      "Assistant: Certainly! Tokyo, originally known as Edo, has a rich history that spans over 1,000 years. Here are some key points in its historical timeline:\n",
      "\n",
      "1. **Early History (Edo Period) (1603-1868)**:\n",
      "   - In 1603, Tokugawa Ieyasu established Edo as the capital of the Tokugawa shogunate, marking the beginning of the Edo period.\n",
      "   - Edo became the center of political, economic, and cultural life in Japan. It was a time of relative peace and stability, known as the Pax Tokugawa.\n",
      "   - The city developed into a vibrant urban center, surrounded by a massive wall and a series of moats. It became a bustling hub of trade, culture, and innovation.\n",
      "\n",
      "2. **Meiji Restoration (1868-1912)**:\n",
      "   - In 1868, Emperor Meiji was restored to the throne, and the capital moved from Edo to a new location, Kyoto, before eventually settling in Tokyo.\n",
      "   - The Meiji Restoration marked the end of the Tokugawa shogunate and the beginning of the Meiji era, characterized by rapid modernization and westernization.\n",
      "   - Tokyo became the political and cultural heart of Japan during this period, witnessing significant industrial and infrastructural development.\n",
      "\n",
      "3. **Modern Tokyo (Post-World War II)**:\n",
      "   - After World War II, Tokyo was heavily damaged but underwent rapid reconstruction and redevelopment.\n",
      "   - In 1943, Tokyo was renamed from its old name, Edo, to Shinjuku, but the name Tokyo was reinstated in 1951.\n",
      "   - Post-war Tokyo saw significant economic growth and became a global financial center. The city expanded rapidly, incorporating surrounding areas and becoming a sprawling metropolis.\n",
      "   - Tokyo has continued to be a major global hub for business, technology, culture, and entertainment, with landmarks such as the Tokyo Tower, Shibuya Crossing, and the Imperial Palace.\n",
      "\n",
      "Today, Tokyo is a dynamic and diverse city that blends traditional Japanese culture with cutting-edge technology and modern conveniences. Its history continues to shape its identity as one of the world's most influential and iconic cities. If you have any more questions or need further details, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "#### Your Task ####\n",
    "## Run a LLM single line query with POST, and add chat history (history stored on the client side only)\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# URL for the chat endpoint\n",
    "url = \"http://localhost:54223/chat\"\n",
    "\n",
    "# Initialize chat history\n",
    "chat_history = []\n",
    "\n",
    "def send_chat_message(prompt):\n",
    "    \"\"\"Send a message to the LLM API and update chat history\"\"\"\n",
    "    global chat_history\n",
    "    \n",
    "    # Prepare the request payload\n",
    "    payload = {\n",
    "        \"prompt\": prompt,\n",
    "        \"history\": chat_history\n",
    "    }\n",
    "    \n",
    "    # Send the POST request\n",
    "    response = requests.post(url, json=payload)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        assistant_message = response.json()  # The API returns the response as a string\n",
    "        \n",
    "        # Update chat history with user's message and assistant's response\n",
    "        chat_history.append({\"role\": \"user\", \"content\": prompt})\n",
    "        chat_history.append({\"role\": \"assistant\", \"content\": assistant_message})\n",
    "        \n",
    "        print(f\"User: {prompt}\")\n",
    "        print(f\"Assistant: {assistant_message}\")\n",
    "        print(\"-\" * 50)\n",
    "    else:\n",
    "        print(f\"Error: Status code {response.status_code}\")\n",
    "        print(response.text)\n",
    "\n",
    "# Example usage - send multiple messages to build history\n",
    "send_chat_message(\"Hello, who are you?\")\n",
    "send_chat_message(\"What's the capital of Japan?\")\n",
    "send_chat_message(\"Can you tell me more about its history?\")\n",
    "\n",
    "# Display the full conversation history\n",
    "print(\"Full conversation history:\")\n",
    "for message in chat_history:\n",
    "    print(f\"{message['role'].capitalize()}: {message['content']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Creating OpenAI-Compatible API server using vLLM\n",
    "\n",
    "In the previous section, we have created a simple API server using FastAPI. However, the OpenAI-like API has been de facto standard for LLM services. Manual implementation of the OpenAI API is tedious. Luckily, there are many open-source frameworks that provide OpenAI-compatible APIs. In this section, we will use vLLM to create an OpenAI-compatible API server.\n",
    "\n",
    "vLLM is a high-throughput and memory-efficient inference and serving engine for LLMs. It uses a novel GPU memory management technique called \"PagedAttention\" to enable efficient inference of large models.\n",
    "\n",
    "vLLM has two modess: Offline Inference and OpenAI-Compatible Server:\n",
    "- **Offline Inference**: This mode is just like the huggingface transformers library. You can load a model and run inference by using vllm as a library.\n",
    "- **OpenAI-Compatible Server**: This mode provides endpoints compatible with the OpenAI API, allowing you to run your own LLMs with a similar interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirrors.cernet.edu.cn/pypi/web/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: vllm in /usr/local/lib/python3.10/dist-packages (0.8.3)\n",
      "Requirement already satisfied: cachetools in /usr/local/lib/python3.10/dist-packages (from vllm) (5.5.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from vllm) (6.0.0)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from vllm) (0.2.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from vllm) (1.26.4)\n",
      "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from vllm) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from vllm) (4.66.5)\n",
      "Requirement already satisfied: blake3 in /usr/local/lib/python3.10/dist-packages (from vllm) (1.0.4)\n",
      "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from vllm) (9.0.0)\n",
      "Requirement already satisfied: transformers>=4.51.0 in /usr/local/lib/python3.10/dist-packages (from vllm) (4.51.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.30.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub[hf_xet]>=0.30.0->vllm) (0.30.1)\n",
      "Requirement already satisfied: tokenizers>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from vllm) (0.21.1)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from vllm) (5.29.4)\n",
      "Requirement already satisfied: fastapi>=0.115.0 in /usr/local/lib/python3.10/dist-packages (from fastapi[standard]>=0.115.0->vllm) (0.115.9)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from vllm) (3.10.5)\n",
      "Requirement already satisfied: openai>=1.52.0 in /usr/local/lib/python3.10/dist-packages (from vllm) (1.70.0)\n",
      "Requirement already satisfied: pydantic>=2.9 in /usr/local/lib/python3.10/dist-packages (from vllm) (2.9.2)\n",
      "Requirement already satisfied: prometheus_client>=0.18.0 in /usr/local/lib/python3.10/dist-packages (from vllm) (0.21.0)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from vllm) (10.4.0)\n",
      "Requirement already satisfied: prometheus-fastapi-instrumentator>=7.0.0 in /usr/local/lib/python3.10/dist-packages (from vllm) (7.1.0)\n",
      "Requirement already satisfied: tiktoken>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from vllm) (0.9.0)\n",
      "Requirement already satisfied: lm-format-enforcer<0.11,>=0.10.11 in /usr/local/lib/python3.10/dist-packages (from vllm) (0.10.11)\n",
      "Requirement already satisfied: llguidance<0.8.0,>=0.7.9 in /usr/local/lib/python3.10/dist-packages (from vllm) (0.7.13)\n",
      "Requirement already satisfied: outlines==0.1.11 in /usr/local/lib/python3.10/dist-packages (from vllm) (0.1.11)\n",
      "Requirement already satisfied: lark==1.2.2 in /usr/local/lib/python3.10/dist-packages (from vllm) (1.2.2)\n",
      "Requirement already satisfied: xgrammar==0.1.17 in /usr/local/lib/python3.10/dist-packages (from vllm) (0.1.17)\n",
      "Requirement already satisfied: typing_extensions>=4.10 in /usr/local/lib/python3.10/dist-packages (from vllm) (4.12.2)\n",
      "Requirement already satisfied: filelock>=3.16.1 in /usr/local/lib/python3.10/dist-packages (from vllm) (3.16.1)\n",
      "Requirement already satisfied: partial-json-parser in /usr/local/lib/python3.10/dist-packages (from vllm) (0.2.1.1.post5)\n",
      "Requirement already satisfied: pyzmq in /usr/local/lib/python3.10/dist-packages (from vllm) (26.2.0)\n",
      "Requirement already satisfied: msgspec in /usr/local/lib/python3.10/dist-packages (from vllm) (0.19.0)\n",
      "Requirement already satisfied: gguf==0.10.0 in /usr/local/lib/python3.10/dist-packages (from vllm) (0.10.0)\n",
      "Requirement already satisfied: importlib_metadata in /usr/local/lib/python3.10/dist-packages (from vllm) (7.2.1)\n",
      "Requirement already satisfied: mistral_common>=1.5.4 in /usr/local/lib/python3.10/dist-packages (from mistral_common[opencv]>=1.5.4->vllm) (1.5.4)\n",
      "Requirement already satisfied: opencv-python-headless>=4.11.0 in /usr/local/lib/python3.10/dist-packages (from vllm) (4.11.0.86)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from vllm) (6.0.2)\n",
      "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from vllm) (0.8.0)\n",
      "Requirement already satisfied: compressed-tensors==0.9.2 in /usr/local/lib/python3.10/dist-packages (from vllm) (0.9.2)\n",
      "Requirement already satisfied: depyf==0.18.0 in /usr/local/lib/python3.10/dist-packages (from vllm) (0.18.0)\n",
      "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from vllm) (3.0.0)\n",
      "Requirement already satisfied: watchfiles in /usr/local/lib/python3.10/dist-packages (from vllm) (1.0.4)\n",
      "Requirement already satisfied: python-json-logger in /usr/local/lib/python3.10/dist-packages (from vllm) (2.0.7)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from vllm) (1.14.0)\n",
      "Requirement already satisfied: ninja in /usr/local/lib/python3.10/dist-packages (from vllm) (1.11.1.1)\n",
      "Requirement already satisfied: numba==0.61 in /usr/local/lib/python3.10/dist-packages (from vllm) (0.61.0)\n",
      "Requirement already satisfied: ray!=2.44.*,>=2.43.0 in /usr/local/lib/python3.10/dist-packages (from ray[cgraph]!=2.44.*,>=2.43.0->vllm) (2.43.0)\n",
      "Requirement already satisfied: torch==2.6.0 in /usr/local/lib/python3.10/dist-packages (from vllm) (2.6.0)\n",
      "Requirement already satisfied: torchaudio==2.6.0 in /usr/local/lib/python3.10/dist-packages (from vllm) (2.6.0)\n",
      "Requirement already satisfied: torchvision==0.21.0 in /usr/local/lib/python3.10/dist-packages (from vllm) (0.21.0)\n",
      "Requirement already satisfied: xformers==0.0.29.post2 in /usr/local/lib/python3.10/dist-packages (from vllm) (0.0.29.post2)\n",
      "Requirement already satisfied: astor in /usr/local/lib/python3.10/dist-packages (from depyf==0.18.0->vllm) (0.8.1)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from depyf==0.18.0->vllm) (0.3.8)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba==0.61->vllm) (0.44.0)\n",
      "Requirement already satisfied: interegular in /usr/local/lib/python3.10/dist-packages (from outlines==0.1.11->vllm) (0.3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from outlines==0.1.11->vllm) (3.1.6)\n",
      "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.10/dist-packages (from outlines==0.1.11->vllm) (1.6.0)\n",
      "Requirement already satisfied: diskcache in /usr/local/lib/python3.10/dist-packages (from outlines==0.1.11->vllm) (5.6.3)\n",
      "Requirement already satisfied: referencing in /usr/local/lib/python3.10/dist-packages (from outlines==0.1.11->vllm) (0.35.1)\n",
      "Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from outlines==0.1.11->vllm) (4.23.0)\n",
      "Requirement already satisfied: pycountry in /usr/local/lib/python3.10/dist-packages (from outlines==0.1.11->vllm) (24.6.1)\n",
      "Requirement already satisfied: airportsdata in /usr/local/lib/python3.10/dist-packages (from outlines==0.1.11->vllm) (20250224)\n",
      "Requirement already satisfied: outlines_core==0.1.26 in /usr/local/lib/python3.10/dist-packages (from outlines==0.1.11->vllm) (0.1.26)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.6.0->vllm) (3.3)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.6.0->vllm) (2024.6.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch==2.6.0->vllm) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch==2.6.0->vllm) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch==2.6.0->vllm) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.10/dist-packages (from torch==2.6.0->vllm) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.10/dist-packages (from torch==2.6.0->vllm) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.10/dist-packages (from torch==2.6.0->vllm) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.10/dist-packages (from torch==2.6.0->vllm) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.10/dist-packages (from torch==2.6.0->vllm) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.10/dist-packages (from torch==2.6.0->vllm) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.10/dist-packages (from torch==2.6.0->vllm) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.10/dist-packages (from torch==2.6.0->vllm) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch==2.6.0->vllm) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch==2.6.0->vllm) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.6.0->vllm) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.6.0->vllm) (1.13.1)\n",
      "Requirement already satisfied: nanobind>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from xgrammar==0.1.17->vllm) (2.6.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch==2.6.0->vllm) (1.3.0)\n",
      "Requirement already satisfied: starlette<0.46.0,>=0.40.0 in /usr/local/lib/python3.10/dist-packages (from fastapi>=0.115.0->fastapi[standard]>=0.115.0->vllm) (0.45.3)\n",
      "Requirement already satisfied: fastapi-cli>=0.0.5 in /usr/local/lib/python3.10/dist-packages (from fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.0.7)\n",
      "Requirement already satisfied: httpx>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from fastapi[standard]>=0.115.0->vllm) (0.27.2)\n",
      "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.10/dist-packages (from fastapi[standard]>=0.115.0->vllm) (0.0.20)\n",
      "Requirement already satisfied: email-validator>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from fastapi[standard]>=0.115.0->vllm) (2.2.0)\n",
      "Requirement already satisfied: uvicorn>=0.12.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.34.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.30.0->huggingface-hub[hf_xet]>=0.30.0->vllm) (23.2)\n",
      "Requirement already satisfied: hf-xet>=0.1.4 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub[hf_xet]>=0.30.0->vllm) (1.0.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.52.0->vllm) (4.6.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.52.0->vllm) (1.7.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.52.0->vllm) (0.9.0)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai>=1.52.0->vllm) (1.3.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.9->vllm) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.9->vllm) (2.23.4)\n",
      "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from ray!=2.44.*,>=2.43.0->ray[cgraph]!=2.44.*,>=2.43.0->vllm) (8.1.7)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ray!=2.44.*,>=2.43.0->ray[cgraph]!=2.44.*,>=2.43.0->vllm) (1.0.8)\n",
      "Requirement already satisfied: aiosignal in /usr/local/lib/python3.10/dist-packages (from ray!=2.44.*,>=2.43.0->ray[cgraph]!=2.44.*,>=2.43.0->vllm) (1.3.1)\n",
      "Requirement already satisfied: frozenlist in /usr/local/lib/python3.10/dist-packages (from ray!=2.44.*,>=2.43.0->ray[cgraph]!=2.44.*,>=2.43.0->vllm) (1.4.1)\n",
      "Requirement already satisfied: cupy-cuda12x in /usr/local/lib/python3.10/dist-packages (from ray[cgraph]!=2.44.*,>=2.43.0->vllm) (13.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->vllm) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->vllm) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->vllm) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->vllm) (2024.8.30)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken>=0.6.0->vllm) (2024.9.11)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.51.0->vllm) (0.4.5)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->vllm) (2.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->vllm) (24.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->vllm) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->vllm) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->vllm) (4.0.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib_metadata->vllm) (3.20.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai>=1.52.0->vllm) (1.2.2)\n",
      "Requirement already satisfied: dnspython>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from email-validator>=2.0.0->fastapi[standard]>=0.115.0->vllm) (2.7.0)\n",
      "Requirement already satisfied: typer>=0.12.3 in /usr/local/lib/python3.10/dist-packages (from fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.12.5)\n",
      "Requirement already satisfied: rich-toolkit>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.14.1)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.23.0->fastapi[standard]>=0.115.0->vllm) (1.0.6)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.23.0->fastapi[standard]>=0.115.0->vllm) (0.14.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->outlines==0.1.11->vllm) (2.1.5)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->outlines==0.1.11->vllm) (2023.12.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->outlines==0.1.11->vllm) (0.20.0)\n",
      "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.6.4)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (1.1.0)\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.21.0)\n",
      "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (15.0.1)\n",
      "Requirement already satisfied: fastrlock>=0.5 in /usr/local/lib/python3.10/dist-packages (from cupy-cuda12x->ray[cgraph]!=2.44.*,>=2.43.0->vllm) (0.8.2)\n",
      "Requirement already satisfied: rich>=13.7.1 in /usr/local/lib/python3.10/dist-packages (from rich-toolkit>=0.11.1->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (13.7.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.12.3->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (1.5.4)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=13.7.1->rich-toolkit>=0.11.1->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=13.7.1->rich-toolkit>=0.11.1->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=13.7.1->rich-toolkit>=0.11.1->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.1.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install vllm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Offline Inference\n",
    "\n",
    "The offline API is based on the LLM class. To initialize the vLLM engine, create a new instance of LLM and specify the model to run.\n",
    "\n",
    "The LLM class provides various methods for offline inference. See Engine Arguments for a list of options when initializing the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-15 15:07:19 [__init__.py:239] Automatically detected platform cuda.\n",
      "INFO 05-15 15:07:30 [config.py:600] This model supports multiple tasks: {'reward', 'embed', 'generate', 'classify', 'score'}. Defaulting to 'generate'.\n",
      "INFO 05-15 15:07:31 [config.py:1780] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 05-15 15:07:32 [core.py:61] Initializing a V1 LLM engine (v0.8.3) with config: model='/ssdshare/share/model/Qwen3-0.6B-Base', speculative_config=None, tokenizer='/ssdshare/share/model/Qwen3-0.6B-Base', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/ssdshare/share/model/Qwen3-0.6B-Base, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":3,\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":512}\n",
      "WARNING 05-15 15:07:33 [utils.py:2413] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f6076ebab30>\n",
      "INFO 05-15 15:07:34 [parallel_state.py:957] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "INFO 05-15 15:07:34 [cuda.py:221] Using Flash Attention backend on V1 engine.\n",
      "INFO 05-15 15:07:34 [gpu_model_runner.py:1258] Starting to load model /ssdshare/share/model/Qwen3-0.6B-Base...\n",
      "WARNING 05-15 15:07:34 [utils.py:76] Qwen3ForCausalLM has no vLLM implementation, falling back to Transformers implementation. Some features may not be supported and performance may not be optimal.\n",
      "INFO 05-15 15:07:34 [transformers.py:118] Using Transformers backend.\n",
      "WARNING 05-15 15:07:34 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6bafc1c91bd4f15bc91408f409c0340",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-15 15:07:41 [loader.py:447] Loading weights took 6.56 seconds\n",
      "INFO 05-15 15:07:42 [gpu_model_runner.py:1273] Model loading took 1.1103 GiB and 7.613681 seconds\n",
      "INFO 05-15 15:07:51 [backends.py:416] Using cache directory: /root/.cache/vllm/torch_compile_cache/c193686457/rank_0_0 for vLLM's torch.compile\n",
      "INFO 05-15 15:07:51 [backends.py:426] Dynamo bytecode transform time: 9.87 s\n",
      "INFO 05-15 15:08:00 [backends.py:132] Cache the graph of shape None for later use\n",
      "INFO 05-15 15:08:31 [backends.py:144] Compiling a graph for general shape takes 39.18 s\n",
      "INFO 05-15 15:08:49 [monitor.py:33] torch.compile takes 49.05 s in total\n",
      "INFO 05-15 15:08:50 [kv_cache_utils.py:578] GPU KV cache size: 79,584 tokens\n",
      "INFO 05-15 15:08:50 [kv_cache_utils.py:581] Maximum concurrency for 32,768 tokens per request: 2.43x\n",
      "INFO 05-15 15:09:19 [gpu_model_runner.py:1608] Graph capturing finished in 29 secs, took 0.48 GiB\n",
      "INFO 05-15 15:09:19 [core.py:162] init engine (profile, create kv cache, warmup model) took 97.75 seconds\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM\n",
    "\n",
    "llm = LLM(\n",
    "    model=\"/ssdshare/share/model/Qwen3-0.6B-Base\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In vLLM, generative models implement the VllmModelForTextGeneration interface. Based on the final hidden states of the input, these models output log probabilities of the tokens to generate, which are then passed through Sampler to obtain the final text.\n",
    "\n",
    "The `generate` method is available to all generative models in vLLM. It is similar to its counterpart in HF Transformers, except that tokenization and detokenization are also performed automatically.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-15 15:16:18 [config.py:1088] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  4.89it/s, est. speed input: 39.44 toks/s, output: 88.72 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: 'Which city is the capital of China?', Generated text: ' Beijing\\n\\nCapitalize this past sentence correctly. The capital city of China is Beijing.'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "outputs = llm.generate(\"Which city is the capital of China?\")\n",
    "\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can optionally control the language generation by passing SamplingParams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.08s/it, est. speed input: 7.41 toks/s, output: 118.55 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: 'Which city is the capital of China?', Generated text: ' Also, what are the capitals of India, Nepal, and Bhutan? Additionally, can you tell me about the capital of Bhutan, the currency used in Bhutan, and the currency of Japan and South Korea? Furthermore, which country has the highest population, and which country has the highest literacy rate? Who is the current President of India, and which leader of Bhutan has won the Nobel Peace Prize? Lastly, what is the capital of the United Kingdom, and which country has the highest literacy rate?\\n\\nThe capital of China is Beijing. The capital of India is Delhi, the capital of Nepal is Kathmandu, and the'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from vllm import SamplingParams\n",
    "\n",
    "params = SamplingParams(\n",
    "    temperature=0.7,\n",
    "    max_tokens=128,\n",
    ")\n",
    "outputs = llm.generate(\"Which city is the capital of China?\", params)\n",
    "\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chat method implements chat functionality on top of generate. In particular, it accepts input similar to OpenAI Chat Completions API and automatically applies the model’s chat template to format the prompt.\n",
    "\n",
    "In general, only instruction-tuned models have a chat template. Base models may perform poorly as they are not trained to respond to the chat conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-15 15:46:32 [__init__.py:239] Automatically detected platform cuda.\n",
      "INFO 05-15 15:46:34 [config.py:209] Replacing legacy 'type' key with 'rope_type'\n",
      "INFO 05-15 15:46:43 [config.py:600] This model supports multiple tasks: {'generate', 'reward', 'classify', 'embed', 'score'}. Defaulting to 'generate'.\n",
      "INFO 05-15 15:46:43 [config.py:1780] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 05-15 15:46:45 [core.py:61] Initializing a V1 LLM engine (v0.8.3) with config: model='/ssdshare/share/model/Phi-4-mini-instruct', speculative_config=None, tokenizer='/ssdshare/share/model/Phi-4-mini-instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/ssdshare/share/model/Phi-4-mini-instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":3,\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":512}\n",
      "WARNING 05-15 15:46:45 [utils.py:2413] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7fee0c3bf700>\n",
      "INFO 05-15 15:46:46 [parallel_state.py:957] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "INFO 05-15 15:46:46 [cuda.py:221] Using Flash Attention backend on V1 engine.\n",
      "INFO 05-15 15:46:46 [gpu_model_runner.py:1258] Starting to load model /ssdshare/share/model/Phi-4-mini-instruct...\n",
      "WARNING 05-15 15:46:46 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a74b9a2600642c986bb1057598284ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-15 15:46:48 [loader.py:447] Loading weights took 1.50 seconds\n",
      "INFO 05-15 15:46:48 [gpu_model_runner.py:1273] Model loading took 7.1694 GiB and 1.797707 seconds\n",
      "INFO 05-15 15:46:55 [backends.py:416] Using cache directory: /root/.cache/vllm/torch_compile_cache/f3846216f7/rank_0_0 for vLLM's torch.compile\n",
      "INFO 05-15 15:46:55 [backends.py:426] Dynamo bytecode transform time: 7.31 s\n",
      "INFO 05-15 15:46:56 [backends.py:115] Directly load the compiled graph for shape None from the cache\n",
      "INFO 05-15 15:47:03 [monitor.py:33] torch.compile takes 7.31 s in total\n",
      "INFO 05-15 15:47:04 [kv_cache_utils.py:578] GPU KV cache size: 126,576 tokens\n",
      "INFO 05-15 15:47:04 [kv_cache_utils.py:581] Maximum concurrency for 8,192 tokens per request: 15.45x\n",
      "INFO 05-15 15:47:26 [gpu_model_runner.py:1608] Graph capturing finished in 22 secs, took 0.54 GiB\n",
      "INFO 05-15 15:47:26 [core.py:162] init engine (profile, create kv cache, warmup model) took 37.99 seconds\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM\n",
    "import gc\n",
    "\n",
    "# terminate the previous LLM instance to free up memory\n",
    "llm = None\n",
    "gc.collect()\n",
    "llm = LLM(\n",
    "    model=\"/ssdshare/share/model/Phi-4-mini-instruct\",\n",
    "    max_model_len=8192,\n",
    "    max_num_seqs=1,\n",
    "    gpu_memory_utilization=0.5,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-15 15:49:23 [chat_utils.py:396] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:10<00:00, 10.51s/it, est. speed input: 3.33 toks/s, output: 97.43 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: '<|system|>You are a helpful assistant<|end|><|user|>Hello<|end|><|assistant|>Hello! How can I assist you today?<|end|><|user|>Write an long essay about the importance of higher education.<|end|><|assistant|>',\n",
      "\n",
      "Generated text: \"The Importance of Higher Education\\n\\nHigher education, often synonymous with college or university-level education, is a critical component of personal and societal development. As we navigate an ever-evolving global landscape, the significance of higher education has grown exponentially. This essay will explore the multifaceted importance of higher education, highlighting its benefits for individuals, society, and the global economy.\\n\\n**Individual Benefits of Higher Education**\\n\\nHigher education offers numerous benefits for individuals, which can be broadly categorized into personal, professional, and social dimensions. \\n\\n1. **Personal Development**: Higher education fosters critical thinking, creativity, and problem-solving skills. These cognitive abilities enable individuals to analyze situations, make informed decisions, and develop innovative solutions to complex problems. Moreover, higher education encourages self-reflection and self-awareness, allowing individuals to understand themselves better and make well-informed life choices.\\n\\n2. **Professional Advancement**: A higher degree is often a prerequisite for many professional careers. Specialized knowledge and skills acquired through higher education open doors to various job opportunities. Professionals with degrees are more likely to secure well-paying jobs, gain promotions, and achieve career success. The competitive edge provided by higher education can significantly impact an individual's earning potential and job stability.\\n\\n3. **Social Mobility**: Higher education is a powerful tool for social mobility. It provides individuals from diverse backgrounds with the means to break the cycle of poverty and achieve upward mobility. Access to higher education can lead to better job prospects, higher incomes, and improved living standards. By breaking down barriers, higher education promotes equality and fosters a more inclusive society.\\n\\n**Societal Benefits of Higher Education**\\n\\nHigher education extends its benefits beyond individuals, positively impacting society as a whole. Several key areas where higher education contributes to the betterment of society include economic growth, social cohesion, and civic engagement.\\n\\n1. **Economic Growth**: Higher education plays a pivotal role in driving economic growth. Graduates with higher levels of education contribute to the economy by increasing productivity, fostering innovation, and promoting entrepreneurship. A well-educated workforce is essential for the development of advanced industries, such as technology, healthcare, and engineering, which are critical for economic prosperity. Higher education also stimulates job creation, as educated individuals are more likely to start their own businesses, creating employment opportunities for others.\\n\\n2. **Social Cohesion**: Higher education promotes social cohesion by fostering a sense of community and shared values among individuals from diverse backgrounds. Universities and colleges often serve as melting pots where people from different cultures, ethnicities, and social classes come together, learn from each other, and develop mutual respect. This shared experience helps bridge social divides, promote tolerance, and reduce prejudice. A cohesive society, characterized by understanding and cooperation, is better equipped to address social challenges and build a harmonious future.\\n\\n3. **Civic Engagement**: Higher education encourages civic engagement by equipping individuals with the knowledge and skills necessary to participate actively in democratic processes. Educated citizens are more likely to be informed about social and political issues, engage in public discourse, and contribute to decision-making processes. Higher education also fosters a sense of responsibility towards the community and society at large, inspiring individuals to advocate for positive change and work towards a better future.\\n\\n**Global Impact of Higher Education**\\n\\nIn an increasingly interconnected world, the global impact of higher education cannot be overstated. Higher education institutions play a crucial role in addressing global challenges, promoting international collaboration, and shaping the future of our planet.\\n\\n1. **Addressing Global Challenges**: Higher education institutions are at the forefront of researching and addressing global challenges such as climate change, poverty, and health crises. Universities and colleges collaborate with governments, non-governmental organizations, and international agencies to develop innovative solutions and implement effective strategies. By leveraging the expertise and resources of higher education, we can work towards a sustainable and equitable future for all.\\n\\n2. **Promoting International Collaboration**: Higher education fosters international collaboration by bringing together students, researchers, and professionals from diverse backgrounds. This exchange of knowledge and ideas enriches the academic experience and promotes cross-cultural understanding. International collaborations in higher education often lead to groundbreaking discoveries, technological advancements, and the development of new solutions to global problems.\\n\\n3. **Shaping the Future**: Higher education institutions are instrumental in shaping the future of our planet. Through research, innovation, and the cultivation of future leaders, universities and colleges contribute to the development of new technologies, sustainable practices, and progressive policies. By nurturing the next generation of thinkers, innovators, and leaders, higher education institutions play a vital role in guiding our collective journey towards a better future.\\n\\n**Conclusion**\\n\\nIn conclusion, the importance of higher education cannot be overstated. It is a transformative experience that profoundly impacts individuals, society, and the global community. Higher education fosters personal growth, professional advancement, and social mobility, while also driving economic growth, promoting social cohesion, and encouraging civic engagement. As we continue to face global challenges and strive for a better future, the role of higher education in shaping the course of human progress remains indispensable. By investing in higher education, we invest in the potential of individuals, the prosperity\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful assistant\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Hello\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"Hello! How can I assist you today?\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Write an long essay about the importance of higher education.\",\n",
    "    },\n",
    "]\n",
    "from vllm import SamplingParams\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0.7,\n",
    "    max_tokens=1024,\n",
    ")\n",
    "outputs = llm.chat(conversation, sampling_params)\n",
    "\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"Prompt: {prompt!r},\\n\\nGenerated text: {generated_text!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1048"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = None\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 OpenAI-Compatible Server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can start the server via the vllm serve command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run it in your terminal\n",
    "# vllm serve /ssdshare/share/model/Phi-4-mini-instruct --dtype auto --api-key token-abc123 --max-model-len 16384"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can use OpenAI python package to access the endpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(\n",
    "    base_url=\"http://localhost:8000/v1\",\n",
    "    api_key=\"token-abc123\",\n",
    ")\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"/ssdshare/share/model/Phi-4-mini-instruct\",\n",
    "  messages=[\n",
    "    {\"role\": \"user\", \"content\": \"Hello!\"}\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vllm provides a set of endpoints that are compatible with OpenAI API, like completion, chat completion, embedding, and so on. You can find the full list of endpoints in the vllm documentation.\n",
    "\n",
    "Moreover, vllm also provides a set of metrics endpoints that can be used to monitor the state and performance of the server.\n",
    "Some of the metrics are: TTFT, TPOT.\n",
    "\n",
    "TTFT is the time it takes to generate the first token of the response. TPOT is the time it takes to generate each token of the response. These metrics are so-called SLO (Service Level Objective) metrics, which are used to measure the performance of the server. VLLM did a lot of work to optimize these SLO."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Adding a Web User Interface using `gradio`\n",
    "\n",
    "Demo a machine learning application is important. It gives the users a direct experience of your algorithm in an interactive manner. Here we'll be building an interesting demo using `gradio`, a popular Python library for ML demos. Let's install this library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Basic Gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirrors.cernet.edu.cn/pypi/web/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: gradio in /usr/local/lib/python3.10/dist-packages (5.23.3)\n",
      "Collecting gradio\n",
      "  Downloading https://mirrors4.bfsu.edu.cn/pypi/web/packages/e2/28/f36287c69c2944c13944e2fc6752a9e673cb4841d2e1217f078c3f403a0b/gradio-5.29.1-py3-none-any.whl (54.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.1/54.1 MB\u001b[0m \u001b[31m70.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (23.2.1)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.6.0)\n",
      "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.115.9)\n",
      "Requirement already satisfied: ffmpy in /usr/local/lib/python3.10/dist-packages (from gradio) (0.5.0)\n",
      "Collecting gradio-client==1.10.1 (from gradio)\n",
      "  Downloading https://mirrors4.bfsu.edu.cn/pypi/web/packages/55/6f/03eb8e0e0ec80eced5ed35a63376dabfc7391b1538502f8e85e9dc5bab02/gradio_client-1.10.1-py3-none-any.whl (323 kB)\n",
      "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.1.2)\n",
      "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.27.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.30.1)\n",
      "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.6)\n",
      "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.5)\n",
      "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.26.4)\n",
      "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.10.16)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (23.2)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.2.2)\n",
      "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (10.4.0)\n",
      "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.9.2)\n",
      "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.0.20)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.2)\n",
      "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.11.4)\n",
      "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.1.6)\n",
      "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.10.0)\n",
      "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.45.3)\n",
      "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.13.2)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.5)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.12.2)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.34.0)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.10.1->gradio) (2024.6.1)\n",
      "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.10.1->gradio) (15.0.1)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (3.7)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.0.6)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.16.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.66.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2023.4)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.23.4)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.7.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.16.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.0.7)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
      "Installing collected packages: gradio-client, gradio\n",
      "  Attempting uninstall: gradio-client\n",
      "    Found existing installation: gradio_client 1.8.0\n",
      "    Uninstalling gradio_client-1.8.0:\n",
      "      Successfully uninstalled gradio_client-1.8.0\n",
      "  Attempting uninstall: gradio\n",
      "    Found existing installation: gradio 5.23.3\n",
      "    Uninstalling gradio-5.23.3:\n",
      "      Successfully uninstalled gradio-5.23.3\n",
      "Successfully installed gradio-5.29.1 gradio-client-1.10.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install gradio --upgrade"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we are able to write an example UI that takes in a text string and output a processed string. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /tmp/gradio_example.py\n"
     ]
    }
   ],
   "source": [
    "%%file /tmp/gradio_example.py\n",
    "\n",
    "import gradio as gr\n",
    "\n",
    "def greet(name, intensity):\n",
    "    return \"Hello, hello \" + name + \"!\" * int(intensity)\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=greet,\n",
    "    inputs=[\"text\", \"slider\"],\n",
    "    outputs=[\"text\"],\n",
    ")\n",
    "\n",
    "demo.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the gradio server by runnning the following command\n",
    "\n",
    "# python /tmp/gradio_example.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /tmp/gradio_example.py\n"
     ]
    }
   ],
   "source": [
    "%%file /tmp/gradio_example.py\n",
    "## Add the port forwarding (port 7860 by default), and you can see http://localhost:7860 in your browser\n",
    "\n",
    "## Try change the last line (launch) to \n",
    "\n",
    "## demo.launch(share=True) \n",
    "## observe the output and see the link to open (without the need of port forwarding)\n",
    "\n",
    "import gradio as gr\n",
    "\n",
    "def greet(name, intensity):\n",
    "    return \"Hello, hello \" + name + \"!\" * int(intensity)\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=greet,\n",
    "    inputs=[\"text\", \"slider\"],\n",
    "    outputs=[\"text\"],\n",
    ")\n",
    "\n",
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 The ChatInterface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /tmp/gradio_example.py\n"
     ]
    }
   ],
   "source": [
    "%%file /tmp/gradio_example.py\n",
    "\n",
    "import random\n",
    "\n",
    "def random_response(message, history):\n",
    "    return random.choice([\"Yes\", \"No\"])\n",
    "\n",
    "import gradio as gr\n",
    "gr.ChatInterface(random_response).launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kill your previous process, and restart the new process\n",
    "\n",
    "# python /tmp/gradio_example.py\n",
    "\n",
    "## Add the port forwarding (port 7860 by default), and you can see http://localhost:7860 in your browser\n",
    "## If you do not kill the previous one, the port number will change to 7861 automatically. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Quick and dirty way of creating a UI for a HuggingFace pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /tmp/simpleui.py\n"
     ]
    }
   ],
   "source": [
    "%%file /tmp/simpleui.py\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import gradio as gr\n",
    "\n",
    "model_path = '/ssdshare/share/model/Phi-4-mini-instruct'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, \n",
    "                                             device_map=\"cuda:0\", \n",
    "                                             torch_dtype=\"auto\", \n",
    "                                             trust_remote_code=True)\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    temperature=0.6,\n",
    "    do_sample=True,\n",
    "    return_full_text=False,\n",
    "    max_new_tokens=500,\n",
    ") \n",
    "gr.Interface.from_pipeline(pipe).launch(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python /tmp/simpleui.py\n",
    "\n",
    "## Add the port forwarding (port 7860 by default), and you can see http://localhost:7860 in your browser\n",
    "## If you do not kill the previous one, the port number will change to 7861 or 7862 automatically. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 A better way to build a web UI for LLM (through an LLM API server)\n",
    "\n",
    "Next, you should implement a script that interact with the Phi-4-mini Chat API server you just created.  \n",
    "\n",
    "Note that you should directly call the API server using request, instead of running the LLM within your UI server process. \n",
    "\n",
    "![Illustration of request](./assets/request.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /tmp/chatUI.py\n"
     ]
    }
   ],
   "source": [
    "%%file /tmp/chatUI.py\n",
    "\n",
    "import gradio as gr\n",
    "import requests\n",
    "import json\n",
    "\n",
    "\n",
    "#### Your Task ####\n",
    "# Insert code here to perform the inference\n",
    "# You can use either the hand-crafted API server or the OpenAI-compatible vLLM server\n",
    "#### End Task ####\n",
    "def predict(message, history):\n",
    "    # Convert Gradio history format to API history format\n",
    "    api_history = []\n",
    "    \n",
    "    for user_msg, assistant_msg in history:\n",
    "        api_history.append({\"role\": \"user\", \"content\": user_msg})\n",
    "        api_history.append({\"role\": \"assistant\", \"content\": assistant_msg})\n",
    "    \n",
    "    # Prepare the request payload\n",
    "    payload = {\n",
    "        \"prompt\": message,\n",
    "        \"history\": api_history\n",
    "    }\n",
    "    \n",
    "    # Send POST request to our API server\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            \"http://localhost:54223/chat\", \n",
    "            json=payload,\n",
    "            timeout=60  # Set a reasonable timeout\n",
    "        )\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            # Parse the response as text\n",
    "            return response.json()\n",
    "        else:\n",
    "            return f\"Error: API returned status code {response.status_code}\"\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return f\"Error connecting to API server: {str(e)}\"\n",
    "\n",
    "gr.ChatInterface(predict).launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Do not forget to start your API server (from above, use the /chat API or use the vLLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add the port forwarding (port 7860 by default), and you can see http://localhost:7860 in your browser\n",
    "## If you do not kill the previous one, the port number will change to 7861 or 7862 automatically. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 More Gradio: Streaming and Multi-media\n",
    "\n",
    "Gradio also supports streaming and multi-media input and output.\n",
    "\n",
    "Magic happens from the `streaming=True` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /tmp/transcribe.py\n"
     ]
    }
   ],
   "source": [
    "%%file /tmp/transcribe.py\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "import numpy as np\n",
    "import gradio as gr\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "model_path = \"/ssdshare/share/model/whisper-large-v3-turbo\" # a multi-lingual audio transcription model\n",
    "\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_path, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_path)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "\n",
    "def transcribe(stream, new_chunk):\n",
    "    sr, y = new_chunk\n",
    "\n",
    "    # Convert to mono if stereo\n",
    "    if y.ndim > 1:\n",
    "        y = y.mean(axis=1)\n",
    "\n",
    "    # normalize\n",
    "    y = y.astype(np.float32)\n",
    "    y /= np.max(np.abs(y))\n",
    "\n",
    "    if stream is not None:\n",
    "        stream = np.concatenate([stream, y])\n",
    "    else:\n",
    "        stream = y\n",
    "    return stream, pipe(\n",
    "        {\n",
    "            \"sampling_rate\": sr,\n",
    "            \"raw\": stream,\n",
    "            \"return_timestamps\": True,\n",
    "            \"task\": \"transcribe\",\n",
    "            \"language\": \"chinese\",\n",
    "        }\n",
    "    )[\"text\"]\n",
    "\n",
    "\n",
    "demo = gr.Interface(\n",
    "    transcribe,\n",
    "    [\"state\", gr.Audio(sources=[\"microphone\"], streaming=True)], # note the streaming=True\n",
    "    [\"state\", \"text\"],\n",
    "    live=True,\n",
    "    time_limit=30,\n",
    "    stream_every=0.5,\n",
    ")\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## python /tmp/transcribe.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Build your own Gradio UI\n",
    "\n",
    "Create a separate Gradio UI to serve other models. Maybe an image model in Lab 5, or a translation model cooperated with a transcription model? Explore the Gradio documentation and HuggingFace model cards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /tmp/speech_translator.py\n"
     ]
    }
   ],
   "source": [
    "%%file /tmp/speech_translator.py\n",
    "import torch\n",
    "import numpy as np\n",
    "import gradio as gr\n",
    "from transformers import (\n",
    "    AutoModelForSpeechSeq2Seq, \n",
    "    AutoProcessor, \n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    pipeline\n",
    ")\n",
    "import os\n",
    "import traceback\n",
    "\n",
    "# Set up device and precision\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device set to use {device}\")\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "# Language code mapping for Whisper\n",
    "LANG_CODES = {\n",
    "    \"English\": \"en\",\n",
    "    \"Chinese\": \"zh\",\n",
    "    \"Japanese\": \"ja\",\n",
    "    \"Spanish\": \"es\",\n",
    "    \"French\": \"fr\",\n",
    "    \"German\": \"de\"\n",
    "}\n",
    "\n",
    "try:\n",
    "    # Load speech recognition model\n",
    "    print(\"Loading speech recognition model...\")\n",
    "    speech_model_path = \"/ssdshare/share/model/whisper-large-v3-turbo\"\n",
    "    speech_model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "        speech_model_path, \n",
    "        torch_dtype=torch_dtype, \n",
    "        low_cpu_mem_usage=True, \n",
    "        use_safetensors=True\n",
    "    )\n",
    "    speech_model.to(device)\n",
    "    speech_processor = AutoProcessor.from_pretrained(speech_model_path)\n",
    "\n",
    "    speech_pipe = pipeline(\n",
    "        \"automatic-speech-recognition\",\n",
    "        model=speech_model,\n",
    "        tokenizer=speech_processor.tokenizer,\n",
    "        feature_extractor=speech_processor.feature_extractor,\n",
    "        torch_dtype=torch_dtype,\n",
    "        device=device,\n",
    "    )\n",
    "    print(\"Speech model loaded successfully!\")\n",
    "\n",
    "    # Load translation model\n",
    "    print(\"Loading translation model...\")\n",
    "    model_path = '/ssdshare/share/model/Phi-4-mini-instruct'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        device_map=\"cuda:0\",\n",
    "        torch_dtype=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "\n",
    "    translation_pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        max_new_tokens=300,\n",
    "        temperature=0.3,\n",
    "        do_sample=True,\n",
    "        return_full_text=False\n",
    "    )\n",
    "    print(\"Translation model loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading models: {e}\")\n",
    "    print(traceback.format_exc())\n",
    "    raise e\n",
    "\n",
    "# Define source and target languages\n",
    "LANGUAGES = [\"English\", \"Chinese\", \"Japanese\", \"Spanish\", \"French\", \"German\"]\n",
    "\n",
    "def process_audio(audio, source_lang, target_lang):\n",
    "    try:\n",
    "        # Check if audio input exists\n",
    "        if audio is None:\n",
    "            return \"Please record or upload audio first.\", \"No audio input detected.\"\n",
    "            \n",
    "        # Debug the audio input structure\n",
    "        print(f\"Audio input type: {type(audio)}\")\n",
    "        print(f\"Audio input: {audio}\")\n",
    "        \n",
    "        # Handle different audio input formats\n",
    "        if isinstance(audio, tuple) and len(audio) == 2:\n",
    "            # Expected format: (sample_rate, audio_data)\n",
    "            sr, y = audio\n",
    "        elif isinstance(audio, str):\n",
    "            # Path to audio file\n",
    "            import soundfile as sf\n",
    "            y, sr = sf.read(audio)\n",
    "        else:\n",
    "            # Try to handle other formats or print detailed error\n",
    "            return f\"Unsupported audio format: {type(audio)}\", \"Audio processing failed.\"\n",
    "        \n",
    "        # Convert to mono if stereo\n",
    "        if y.ndim > 1:\n",
    "            y = y.mean(axis=1)\n",
    "        \n",
    "        # Normalize audio\n",
    "        y = y.astype(np.float32)\n",
    "        if np.max(np.abs(y)) > 0:\n",
    "            y /= np.max(np.abs(y))\n",
    "        \n",
    "        # Get language code for Whisper\n",
    "        lang_code = LANG_CODES.get(source_lang, \"en\")\n",
    "        \n",
    "        print(f\"Transcribing audio in {source_lang} ({lang_code})...\")\n",
    "        # Transcribe with Whisper\n",
    "        transcription_result = speech_pipe(\n",
    "            {\"sampling_rate\": sr, \"raw\": y},\n",
    "            task=\"transcribe\",\n",
    "            language=lang_code,\n",
    "            return_timestamps=False\n",
    "        )\n",
    "        \n",
    "        transcribed_text = transcription_result[\"text\"].strip()\n",
    "        print(f\"Transcription result: {transcribed_text}\")\n",
    "        \n",
    "        if not transcribed_text:\n",
    "            return \"Couldn't detect any speech in the recording.\", \"No speech detected.\"\n",
    "        \n",
    "        # Skip translation if source and target languages are the same\n",
    "        if source_lang == target_lang:\n",
    "            return transcribed_text, transcribed_text\n",
    "        \n",
    "        print(f\"Translating from {source_lang} to {target_lang}...\")\n",
    "        # Create a translation prompt for the LLM\n",
    "        translation_prompt = f\"Translate the following text from {source_lang} to {target_lang}: '{transcribed_text}'\"\n",
    "        \n",
    "        # Get translation from LLM\n",
    "        translation_result = translation_pipe(translation_prompt)\n",
    "        translated_text = translation_result[0][\"generated_text\"].strip()\n",
    "        \n",
    "        # Clean up the translation output to extract just the translated content\n",
    "        if \":\" in translated_text:\n",
    "            translated_text = translated_text.split(\":\", 1)[1].strip()\n",
    "        if translated_text.startswith(\"'\") and translated_text.endswith(\"'\"):\n",
    "            translated_text = translated_text[1:-1]\n",
    "            \n",
    "        print(f\"Translation result: {translated_text}\")\n",
    "        return transcribed_text, translated_text\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in process_audio: {e}\")\n",
    "        print(traceback.format_exc())\n",
    "        return f\"Error: {str(e)}\", f\"Translation failed: {str(e)}\"\n",
    "\n",
    "# Create the Gradio interface\n",
    "with gr.Blocks(title=\"Speech Translator\") as demo:\n",
    "    gr.Markdown(\"# 🎙️ Speech-to-Text Translation 🌐\")\n",
    "    gr.Markdown(\"Record your speech, and it will be transcribed and translated to your target language.\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        source_lang = gr.Dropdown(choices=LANGUAGES, value=\"English\", label=\"Source Language\")\n",
    "        target_lang = gr.Dropdown(choices=LANGUAGES, value=\"Chinese\", label=\"Target Language\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        # Changed audio input type to match what Gradio expects\n",
    "        audio_input = gr.Audio(source=\"microphone\", type=\"numpy\", label=\"Record Speech\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        transcribe_btn = gr.Button(\"Transcribe & Translate\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            transcription_output = gr.Textbox(label=\"Transcription\", lines=5)\n",
    "        with gr.Column():\n",
    "            translation_output = gr.Textbox(label=\"Translation\", lines=5)\n",
    "    \n",
    "    # Handle the button click\n",
    "    transcribe_btn.click(\n",
    "        fn=process_audio,\n",
    "        inputs=[audio_input, source_lang, target_lang],\n",
    "        outputs=[transcription_output, translation_output]\n",
    "    )\n",
    "\n",
    "    # Add debug info area\n",
    "    with gr.Accordion(\"Debug Information\", open=False):\n",
    "        debug_info = gr.Textbox(label=\"System Info\", value=f\"Device: {device}, PyTorch: {torch.__version__}\", interactive=False)\n",
    "\n",
    "# Launch the application with specific server configurations\n",
    "demo.launch(\n",
    "    server_name=\"0.0.0.0\",\n",
    "    share=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
